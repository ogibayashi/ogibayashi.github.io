<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tech Notes]]></title>
  <link href="http://ogibayashi.github.io/atom.xml" rel="self"/>
  <link href="http://ogibayashi.github.io/"/>
  <updated>2015-01-09T13:06:09+09:00</updated>
  <id>http://ogibayashi.github.io/</id>
  <author>
    <name><![CDATA[OGIBAYASHI Hironori]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDP2.2をセットアップするためにハマった箇所のメモ]]></title>
    <link href="http://ogibayashi.github.io/blog/2015/01/06/hdp2-dot-2-installation-memo/"/>
    <updated>2015-01-06T20:53:54+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2015/01/06/hdp2-dot-2-installation-memo</id>
    <content type="html"><![CDATA[<p>HDP2.2を手元のVMで試しにセットアップしてみたが、色々ハマった部分があったのでメモ</p>

<h2>環境</h2>

<p>CentOS6.3のVMを7つ用意して、以下のようにHA含めて構成することにした.</p>

<ul>
<li>master1: NameNode(active), ZKFC, JournalNode, Zookeeper</li>
<li>master2: NameNode(standby), ZKFC, JournalNode, ResourceManager(standby), Zookeeper</li>
<li>master3: JournalNode, ResourceManager(active), Zookeeper, HiveServer2, MySQL</li>
<li>slaves(3ノード): DataNode, NodeManager</li>
<li>client: MR/Tez client</li>
</ul>


<p>ドキュメントは、こちらの&#8221;Installing HDP Manually&#8221;を使用.
<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html</a></p>

<h2>トラブルシュートなどのメモ</h2>

<p>以下、ドキュメントには無いが変更しないといけなかったもの、引っかかったトラブルなど. 単に自分が手順を見落としていたり、間違っていたために発生したものもあるかも.</p>

<h3>全般</h3>

<ul>
<li>基本的に、設定はcompanion filesのものをベースにする. 2.1を動かしていた際の設定もあったが、大分変わっているようなので一旦companion filesのをまるごと持ってきた</li>
<li>インストールのベースが<code>/usr/hdp/2.2.0.0-2041/</code>になっているが、実際のスクリプトの中では<code>/usr/lib/hadoop</code>等を参照しているものもあるため、<code>/usr/hdp/2.2.0.0-2041/hadoop -&gt; /usr/hdp/2.2.0.0-2041/hadoop</code> 等のようなシンボリックリンクをひと通り作成した.</li>
<li>Daemonの起動スクリプト類は<code>hadoop-hdfs-namenode</code>等のような別RPMになっている.これらのインストール先は<code>/usr/hdp/2.2.0.0-2041/etc/</code>となっているため、<code>/etc/init.d</code>の下などに、こちらもシンボリックリンクを作成した. ちなみに、<code>/etc/default</code>の下に置くファイルも用意されているが、initスクリプトをみてもこれらを読むようには見えない.</li>
<li>マニュアルにはcompanion filesに含まれる、 <code>usersAndGroups.sh</code>, <code>directories.sh</code> を設定した上で <code>~/.bash_profile</code>でこれらのファイルを読む設定を入れるようにあるが、daemonの動作がbash_profileに依存するのが気持ち悪かったので、それはやってない. それに起因したトラブルもあるかも.</li>
</ul>


<h3>Zookeeperのセットアップ</h3>

<ul>
<li>initスクリプト内で呼ばれる、<code>zookeeper-server</code>,<code>zookeeper-server-initialize</code>は<code>/usr/hdp/2.2.0.0-2041/zookeeper/bin/</code>にあるので、これらを<code>/usr/bin</code>下に置くよう、シンボリックリンクを作成した</li>
</ul>


<h3>service <hadoop daemon> start の戻り値が3</h3>

<p>また、停止に失敗したりする.</p>

<p>以下の様な感じ.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># service hadoop-yarn-resourcemanager start
</span><span class='line'>Starting Hadoop resourcemanager:                           [  OK  ]
</span><span class='line'>starting resourcemanager, logging to /var/log/hadoop/yarn/yarn-yarn-resourcemanager-hdp15.hadoop.local.out
</span><span class='line'>[root@hdp15 init.d]# echo $?
</span><span class='line'>3</span></code></pre></td></tr></table></div></figure>


<p>initスクリプト内で以下のようにPIDFILEを設定しているが、環境変数が正しく設定されていないと、PIDFILEがうまく作られずにこのような状態になる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>PIDFILE="$HADOOP_PID_DIR/yarn-$YARN_IDENT_STRING-resourcemanager.pid"</span></code></pre></td></tr></table></div></figure>


<p><code>yarn-env.sh</code>で<code>HADOOP_PID_DIR</code>, <code>YARN_IDENT_STRING</code>, <code>hadoop-env.sh</code>でも<code>HADOOP_PID_DIR</code>を設定するようにした.</p>

<h3>HistoryServerでPermission Deninedが発生</h3>

<p>以下の様なエラーが発生. (何をしようとして発生したのか忘れた&hellip;)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-12-27 03:15:05,824 ERROR hs.HistoryFileManager (HistoryFileManager.java:scanIfNeeded(285)) - Error while trying to scan the directory hdfs://hdpexperiment:
</span><span class='line'>8020/mr-history/tmp/client
</span><span class='line'>org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=READ_EXECUTE, inode="/mr-history/tmp/client":client:hdfs:drwxrwx---
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:185)</span></code></pre></td></tr></table></div></figure>


<p>HDFSのパーミッションを見ると以下のようになっており、<code>/mr-history/tmp/client</code>に対して<code>mapred</code>ユーザのパーミッションが無い.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># sudo -u hdfs hdfs dfs -ls /
</span><span class='line'>Found 6 items
</span><span class='line'>drwxrwxrwt   - yarn   yarn            0 2014-12-27 03:11 /app-logs
</span><span class='line'>drwxr-xr-x   - hdfs   hdfs            0 2014-12-27 02:43 /apps
</span><span class='line'>drwxr-xr-x   - hdfs   hadoop          0 2014-12-26 16:17 /hdp
</span><span class='line'>drwxr-xr-x   - mapred hdfs            0 2014-12-26 16:16 /mr-history
</span><span class='line'>drwxrwxrwt   - hdfs   hdfs            0 2014-12-27 03:11 /tmp
</span><span class='line'>drwxr-xr-x   - hdfs   hdfs            0 2014-12-27 02:33 /user
</span><span class='line'># sudo -u hdfs hdfs dfs -ls /mr-history
</span><span class='line'>Found 2 items
</span><span class='line'>drwxrwxrwt   - mapred hdfs          0 2014-12-26 16:16 /mr-history/done
</span><span class='line'>drwxrwxrwt   - mapred hdfs          0 2014-12-26 23:27 /mr-history/tmp
</span><span class='line'># sudo -u hdfs hdfs dfs -ls /mr-history/tmp
</span><span class='line'>Found 1 items
</span><span class='line'>drwxrwx---   - client hdfs          0 2014-12-27 00:14 /mr-history/tmp/client
</span></code></pre></td></tr></table></div></figure>


<p>以下のように、<code>/mr-history</code>以下のgroupを<code>mapredと</code>することで対応.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># sudo -u hdfs hdfs dfs -chgrp -R mapred /mr-history</span></code></pre></td></tr></table></div></figure>


<h3>MapReduceジョブ実行中のエラー. Slaveにつながらない</h3>

<p>exampleのpiを実行した際のエラー</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>14/12/30 11:17:59 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:00 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:01 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:29 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:30 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:31 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:31 INFO mapreduce.Job: Job job_1419895534181_0002 failed with state FAILED due to: Application application_1419895534181_0002 failed 2 times due to AM Container for appattempt_1419895534181_0002_000002 exited with  exitCode: 255
</span><span class='line'>For more detailed output, check application tracking page:http://hdp16.hadoop.local:8088/proxy/application_1419895534181_0002/Then, click on links to logs of each attempt.
</span><span class='line'>Diagnostics: Exception from container-launch.
</span><span class='line'>Container id: container_1419895534181_0002_02_000001
</span><span class='line'>Exit code: 255
</span><span class='line'>Stack trace: ExitCodeException exitCode=255: 
</span><span class='line'>        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
</span><span class='line'>        at org.apache.hadoop.util.Shell.run(Shell.java:455)
</span><span class='line'>        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
</span><span class='line'>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Container exited with a non-zero exit code 255
</span><span class='line'>Failing this attempt. Failing the application.
</span><span class='line'>14/12/30 11:18:31 INFO mapreduce.Job: Counters: 0
</span><span class='line'>Job Finished in 99.802 seconds
</span><span class='line'>java.io.FileNotFoundException: File does not exist: hdfs://hdpexperiment/user/hdfs/QuasiMonteCarlo_1419905807487_296085847/out/reduce-out
</span><span class='line'>        at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1122)
</span><span class='line'>        at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
</span><span class='line'>        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
</span><span class='line'>        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
</span><span class='line'>        at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1750)
</span><span class='line'>        at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1774)
</span><span class='line'>        at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:314)
</span><span class='line'>        at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:354)
</span><span class='line'>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
</span><span class='line'>        at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:363)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
</span><span class='line'>        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
</span><span class='line'>        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
</span><span class='line'>        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p><code>yarn logs</code>ではログが見えなかったので、実行中のサーバでコンテナのログを見てみた. <code>${hdp.version}</code>というのがそのままになっている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-12-30 13:08:49,568 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
</span><span class='line'>java.lang.IllegalArgumentException: Unable to parse '/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework' as a URI, check the setting for mapreduce.application.framework.path
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.getMRFrameworkName(MRApps.java:178)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.setMRFrameworkClasspath(MRApps.java:203)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.setClasspath(MRApps.java:248)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInitialClasspath(TaskAttemptImpl.java:620)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:755)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext(TaskAttemptImpl.java:812)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1527)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1504)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1069)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1311)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1303)
</span><span class='line'>        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
</span><span class='line'>        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>Caused by: java.net.URISyntaxException: Illegal character in path at index 11: /hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework
</span><span class='line'>        at java.net.URI$Parser.fail(URI.java:2829)
</span><span class='line'>        at java.net.URI$Parser.checkChars(URI.java:3002)
</span><span class='line'>        at java.net.URI$Parser.parseHierarchical(URI.java:3086)
</span><span class='line'>        at java.net.URI$Parser.parse(URI.java:3044)
</span><span class='line'>        at java.net.URI.&lt;init&gt;(URI.java:595)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.getMRFrameworkName(MRApps.java:176)
</span><span class='line'>        ... 18 more
</span></code></pre></td></tr></table></div></figure>


<p><code>mapred-site.xml</code>で、<code>${hdp.version}</code>となっている部分を実際のバージョン番号(<code>2.2.0.0-2041</code>)に変えたら解消した.</p>

<h3>Journalnodeの停止に失敗</h3>

<p>journalnodeを停止すると、<code>no journalnode to stop</code>というエラーが発生. ただ、プロセスは停止している.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># service hadoop-hdfs-journalnode stop
</span><span class='line'>Stopping Hadoop journalnode:                               [  OK  ]
</span><span class='line'>no journalnode to stop
</span><span class='line'>rm: cannot remove `/var/run/hadoop/hadoop-hdfs-journalnode.pid': Permission denied
</span><span class='line'># ls -l /var/run/hadoop/hadoop-hdfs-journalnode.pid
</span><span class='line'>-rw-r--r-- 1 hdfs hdfs 6 12月 30 08:24 2014 /var/run/hadoop/hadoop-hdfs-journalnode.pid
</span><span class='line'># ps -ef | grep -i journal
</span><span class='line'>root       374     2  0 Dec29 ?        00:00:02 [kjournald]
</span><span class='line'>root       811     2  0 Dec29 ?        00:00:00 [kjournald]
</span><span class='line'>root     14406 14342  0 14:15 pts/0    00:00:00 grep -i journal</span></code></pre></td></tr></table></div></figure>


<p>ディレクトリのowner/groupが<code>mapred</code>になっている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ls -ld /var/run/hadoop
</span><span class='line'>drwxr-xr-x 5 mapred mapred 4096 12月 30 14:16 2014 /var/run/hadoop</span></code></pre></td></tr></table></div></figure>


<p>これを<code>hdfs.hdfs</code>にしたら事象は解消したが、HistoryServerをrestartしたら<code>/var/run/hadoop</code>のownerが<code>mapred.mapred</code>に戻ってしまった.</p>

<p><code>hadoop-mapreduce-historyserver</code>のinitスクリプトにある、以下の部分のせい. つまり、historyserverとHDFS系のdaemonが同居している場合に発生する問題.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>install -d -m 0755 -o mapred -g mapred $HADOOP_PID_DIR 1&gt;/dev/null 2&gt;&1 || :
</span><span class='line'>[ -d "$LOCKDIR" ] || install -d -m 0755 $LOCKDIR 1&gt;/dev/null 2&gt;&1 || :
</span></code></pre></td></tr></table></div></figure>


<p><code>hadoop-env.sh</code>に以下の記述を入れ、HistoryServerの<code>HADOOP_PID_DIR</code>をHDFS系と分けることで対応することにした.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># For HistoryServer
</span><span class='line'>if [ "${SVC_USER}" = "mapred" ]; then
</span><span class='line'>  HADOOP_PID_DIR=/var/run/hadoop-mapreduce
</span><span class='line'>fi
</span></code></pre></td></tr></table></div></figure>


<h3>Hive CREATE TABLE時のNo privilege</h3>

<p>beelineからhiveユーザで接続し、create tableを発行した際のエラー</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0: jdbc:hive2://hdp16.hadoop.local:10000&gt; create table test2(a int, b string); 
</span><span class='line'>Error: Error while compiling statement: FAILED: SemanticException MetaException(message:No privilege 'Select' found for inputs { database:default}) (state=42000,code=40000)</span></code></pre></td></tr></table></div></figure>


<p><a href="https://cwiki.apache.org/confluence/display/Hive/Storage+Based+Authorization+in+the+Metastore+Server">Storage Based Authorization in the Metastore Server</a>に引っかかっている模様</p>

<p>一旦以下の設定を外して対応</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener&lt;/value&gt;
</span><span class='line'>    &lt;description&gt;List of comma separated listeners for metastore events.&lt;/description&gt;
</span><span class='line'>  &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h3>Hive on MRでのクエリ実行エラー</h3>

<p>HiveServer2のログには以下のメッセージが出ている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015-01-06 03:14:42,300 ERROR [HiveServer2-Background-Pool: Thread-65]: exec.Task (SessionState.java:printError(833)) - Ended Job = job_1420474977406_0003 with
</span><span class='line'>exception 'java.lang.NumberFormatException(For input string: "100000L")'
</span><span class='line'>java.lang.NumberFormatException: For input string: "100000L"
</span><span class='line'>        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
</span><span class='line'>        at java.lang.Long.parseLong(Long.java:441)
</span><span class='line'>        at java.lang.Long.parseLong(Long.java:483)
</span><span class='line'>        at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1189)
</span><span class='line'>        at org.apache.hadoop.hive.conf.HiveConf.getLongVar(HiveConf.java:2253)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.checkFatalErrors(HadoopJobExecHelper.java:209)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:308)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:547)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:435)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1604)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1364)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1177)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:999)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:144)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:69)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:196)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at javax.security.auth.Subject.doAs(Subject.java:415)
</span><span class='line'>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
</span><span class='line'>        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:208)
</span><span class='line'>        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
</span><span class='line'>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span></code></pre></td></tr></table></div></figure>


<p><a href="https://issues.apache.org/jira/browse/AMBARI-8219">AMBARI-8219</a>
の事例に従い、hive-site.xmlの以下を変更したらOK</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>   &lt;property&gt;
</span><span class='line'>     &lt;name&gt;hive.exec.max.created.files&lt;/name&gt;
</span><span class='line'>-    &lt;value&gt;100000L&lt;/value&gt;
</span><span class='line'>+    &lt;value&gt;100000&lt;/value&gt;
</span><span class='line'>     &lt;description&gt;Maximum number of HDFS files created by all mappers/reducers in a MapReduce job.&lt;/description&gt;
</span><span class='line'>   &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h3>hive.execution.engine=tez でのHiveクエリ実行エラー1</h3>

<p>HiveServer2のログは以下の通り</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>5841 end=1419741946024 duration=183 from=org.apache.hadoop.hive.ql.Driver&gt;
</span><span class='line'>2014-12-28 13:45:46,025 ERROR [HiveServer2-Handler-Pool: Thread-56]: thrift.ProcessFunction (ProcessFunction.java:process(41)) - Internal error processing ExecuteStatement
</span><span class='line'>java.lang.NoClassDefFoundError: org/apache/tez/dag/api/SessionNotRunning
</span><span class='line'>        at java.lang.Class.getDeclaredConstructors0(Native Method)
</span><span class='line'>        at java.lang.Class.privateGetDeclaredConstructors(Class.java:2585)
</span><span class='line'>        at java.lang.Class.getConstructor0(Class.java:2885)
</span><span class='line'>        at java.lang.Class.newInstance(Class.java:350)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.TaskFactory.get(TaskFactory.java:133)</span></code></pre></td></tr></table></div></figure>


<p>このクラスはtez-apiのjarに含まれているが、HiveServer2のサーバにTezクライアントをセットアップしていなかったのが原因だった. セットアップして解消.</p>

<h3>Tez OrderedWordCountの実行エラー</h3>

<p>Tezの動作確認として、tez-examplesに含まれる、OrderedWordCountを実行した際のエラー.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># su - hdfs
</span><span class='line'>-bash-4.1$ cat /tmp/test.txt 
</span><span class='line'>foo bar foo bar foo
</span><span class='line'>$ hadoop fs -put /tmp/test.txt /tmp/test.txt 
</span><span class='line'>-bash-4.1$ hadoop jar /usr/hdp/2.2.0.0-2041/tez/tez-examples-0.5.2.2.2.0.0-2041.jar orderedwordcount /tmp/test.txt /tmp/out
</span><span class='line'>Running OrderedWordCount
</span><span class='line'>14/12/27 02:54:57 INFO client.TezClient: Tez Client Version: [ component=tez-api, version=0.5.2.2.2.0.0-2041, revision=db32ad437885baf17ab90885b4ddb226fbbe3559, SCM-URL=scm:git:https://git-wip-us.apache.org/repos/asf/tez.git, buildTIme=20141119-1512 ]
</span><span class='line'>14/12/27 02:54:59 INFO client.TezClient: Submitting DAG application with id: application_1419606523103_0010
</span><span class='line'>14/12/27 02:54:59 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
</span><span class='line'>14/12/27 02:54:59 INFO client.TezClientUtils: Using tez.lib.uris value from configuration: hdfs://hdpexperiment/apps/tez/,hdfs://hdpexperiment/apps/tez/lib/,hdfs://hdpexperiment/hdp/apps/current/tez/tez.tar.gz
</span><span class='line'>14/12/27 02:54:59 WARN client.TezClientUtils: Duplicate resource found, resourceName=tez.tar.gz, existingPath=scheme: "hdfs" host: "hdpexperiment" port: -1 file: "/apps/tez/lib/tez.tar.gz", newPath=hdfs://hdpexperiment/hdp/apps/current/tez/tez.tar.gz
</span><span class='line'>14/12/27 02:54:59 INFO client.TezClient: Tez system stage directory hdfs://hdpexperiment/tmp/hdfs/staging/.tez/application_1419606523103_0010 doesn't exist and is created
</span><span class='line'>14/12/27 02:55:00 INFO client.TezClient: Submitting DAG to YARN, applicationId=application_1419606523103_0010, dagName=OrderedWordCount
</span><span class='line'>14/12/27 02:55:01 INFO impl.YarnClientImpl: Submitted application application_1419606523103_0010
</span><span class='line'>14/12/27 02:55:01 INFO client.TezClient: The url to track the Tez AM: http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/
</span><span class='line'>14/12/27 02:55:01 INFO client.DAGClientImpl: Waiting for DAG to start running
</span><span class='line'>14/12/27 02:55:13 INFO client.DAGClientImpl: DAG completed. FinalState=FAILED
</span><span class='line'>OrderedWordCount failed with diagnostics: [Application application_1419606523103_0010 failed 2 times due to AM Container for appattempt_1419606523103_0010_000002 exited with  exitCode: 1
</span><span class='line'>For more detailed output, check application tracking page:http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/Then, click on links to logs of each attempt.
</span><span class='line'>Diagnostics: Exception from container-launch.
</span><span class='line'>Container id: container_1419606523103_0010_02_000001
</span><span class='line'>Exit code: 1
</span><span class='line'>Stack trace: ExitCodeException exitCode=1:
</span><span class='line'>        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
</span><span class='line'>        at org.apache.hadoop.util.Shell.run(Shell.java:455)
</span><span class='line'>        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
</span><span class='line'>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Container exited with a non-zero exit code 1
</span><span class='line'>Failing this attempt. Failing the application.]
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>コンテナのログを見ると以下の通り. クラスが見つからない.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Container: container_1419606523103_0009_01_000001 on hdp17.hadoop.local_45454
</span><span class='line'>===============================================================================
</span><span class='line'>LogType:stderr
</span><span class='line'>Log Upload Time:27-12-2014 03:37:52
</span><span class='line'>LogLength:1445
</span><span class='line'>Log Contents:
</span><span class='line'>Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/service/AbstractService
</span><span class='line'>        at java.lang.ClassLoader.defineClass1(Native Method)
</span><span class='line'>        at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
</span><span class='line'>        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
</span><span class='line'>        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
</span><span class='line'>        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)
</span><span class='line'>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.service.AbstractService
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>        ... 13 more
</span></code></pre></td></tr></table></div></figure>


<p>このクラスは、hadoop-common.jarに入っており、<code>yarn classpath</code>で確認するとこのjarもCLASSPATHに含まれているのだが、、、</p>

<p>(1/9追記)どうやら、<code>tez.lib.uris</code>の問題だった模様.以下のように、HDFSに乗せた<code>tez.tar.gz</code>のパスを指定したらうまく動作した.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;/hdp/apps/current/tez/tez.tar.gz&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h2>まとめ</h2>

<p>ということで、HDP2.2のクラスタを作ろうとして色々うまくいかなかったのでまとめてみた. <del>あとはTezのエラーを解消したいなぁ.</del> あと、マニュアルをブラウザで見るととても見づらいので、PDFをダウンロードして手元で見た方が良い.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentd v0.12のAt-least-once Semanticsを試す]]></title>
    <link href="http://ogibayashi.github.io/blog/2014/12/16/try-fluentd-v0-dot-12-at-least-once/"/>
    <updated>2014-12-16T00:34:17+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2014/12/16/try-fluentd-v0-dot-12-at-least-once</id>
    <content type="html"><![CDATA[<p>Fluentd v0.12のin/out_forwardでAt-least-once semanticsがサポートされるようになった. 今まではアプリケーションレイヤでの到達確認がなかったので、一部のネットワーク障害などのケースでは、送信されたように見えて実は送信されていない、という事象が発生し得た. v0.12から導入された<code>require_ack_response</code>オプションを使うと、このような事象を避けることができる.</p>

<p>この機能が導入されたpull requestはこちら.
<a href="https://github.com/fluent/fluentd/pull/428">https://github.com/fluent/fluentd/pull/428</a></p>

<p>ということで試してみた.</p>

<h2>require_ack_responseがない場合</h2>

<p>fluentd 0.10.56で試す. (0.12で試しても良かったのだけど..)</p>

<p>送信側は以下の設定. 相手先fluentdが早々にdetachされてしまうのを避けるため、<code>hard_timeout</code>と<code>phi_threshold</code>を入れた</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;source&gt;
</span><span class='line'>   type forward
</span><span class='line'>&lt;/source&gt;
</span><span class='line'>&lt;match test.**&gt;
</span><span class='line'>   type forward
</span><span class='line'>   flush_interval 1s
</span><span class='line'>   heartbeat_type tcp
</span><span class='line'>   hard_timeout 600
</span><span class='line'>   phi_threshold 300
</span><span class='line'>   buffer_type file
</span><span class='line'>   buffer_path /var/log/fluentd.*.buffer
</span><span class='line'>   &lt;server&gt;
</span><span class='line'>     host  192.168.1.2
</span><span class='line'>     port 24224
</span><span class='line'>   &lt;/server&gt;
</span><span class='line'>&lt;/match&gt;
</span></code></pre></td></tr></table></div></figure>


<p>受信側はこんな感じ</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;source&gt;
</span><span class='line'>  type forward
</span><span class='line'>&lt;/source&gt;
</span><span class='line'>
</span><span class='line'>&lt;match test.**&gt;
</span><span class='line'>  type file
</span><span class='line'>  path /tmp/fluentd_forward.log
</span><span class='line'>&lt;/match&gt;
</span></code></pre></td></tr></table></div></figure>


<p>で、パケットが届かないが、アプリケーションにエラーが返らない状況を作るため、受信側のiptablesでSYNが立っていないパケットをドロップするようにする. SYNは相手に到達し、SYN-ACKも返るため、アプリケーションからは正常に接続されている様に見えることになる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># iptables -A INPUT -p tcp --syn --dport 24224 -j ACCEPT
</span><span class='line'># iptables -A INPUT -p tcp --dport 24224 -j DROP</span></code></pre></td></tr></table></div></figure>


<p>これでログを送ってみる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># echo '{"aaa": 1}' | fluent-cat  test.data
</span><span class='line'># echo '{"bbb": 2}' | fluent-cat test.data</span></code></pre></td></tr></table></div></figure>


<p>netstatで送信側のソケットを見る. Send-Qにデータが溜まっている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># netstat -na | grep 24224
</span><span class='line'>tcp        0      0 0.0.0.0:24224               0.0.0.0:*                   LISTEN
</span><span class='line'>tcp        0      1 192.168.1.1:10652          192.168.1.2:24224          FIN_WAIT1
</span><span class='line'>tcp        0     41 192.168.1.1:10655          192.168.1.2:24224          FIN_WAIT1
</span><span class='line'>udp        0      0 0.0.0.0:24224               0.0.0.0:*
</span></code></pre></td></tr></table></div></figure>


<p>しばらくすると、ソケットが破棄される.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># netstat -na | grep 24224
</span><span class='line'>tcp        0      0 0.0.0.0:24224               0.0.0.0:*                   LISTEN      
</span><span class='line'>tcp        0      1 192.168.1.1:10664           192.168.1.2:24224          FIN_WAIT1   
</span><span class='line'>udp        0      0 0.0.0.0:24224               0.0.0.0:*             
</span></code></pre></td></tr></table></div></figure>


<p>この状況だと、アプリケーション的には正常に送れているように見えてしまうので、バッファは削除される. つまりログがロストした状況.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ls /var/log/fluentd.*.buffer
</span><span class='line'>ls: cannot access /var/log/fluentd.*.buffer: そのようなファイルやディレクトリはありません
</span></code></pre></td></tr></table></div></figure>


<h2>require_ack_responseを使う</h2>

<p>次に、送受信共に<code>v0.12.1</code>にして、送信側に<code>require_ack_response</code>の設定を入れてみる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;source&gt;
</span><span class='line'>    type forward
</span><span class='line'>  &lt;/source&gt;
</span><span class='line'>  &lt;match test.**&gt;
</span><span class='line'>    type forward
</span><span class='line'>    flush_interval 1s
</span><span class='line'>    heartbeat_type tcp
</span><span class='line'>    hard_timeout 600
</span><span class='line'>    phi_threshold 300
</span><span class='line'>    buffer_type file
</span><span class='line'>    buffer_path /var/log/fluentd.*.buffer
</span><span class='line'>    require_ack_response 
</span><span class='line'>    &lt;server&gt;
</span><span class='line'>      host 192.168.1.2
</span><span class='line'>      port 24224
</span><span class='line'>    &lt;/server&gt;
</span><span class='line'>  &lt;/match&gt;
</span></code></pre></td></tr></table></div></figure>


<p>同様にfluent-catで送る. 今度は、一定時間後に以下のようにエラーになった.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-12-15 15:25:56 +0900 [warn]: no response from 192.168.1.2:24224. regard it as unavailable.
</span><span class='line'>2014-12-15 15:26:56 +0900 [warn]: temporarily failed to flush the buffer. next_retry=2014-12-15 15:22:46 +0900 error_class="Fluent::ForwardOutputACKTimeoutError" error="node 10.29.254.66:24224 does not return ACK" plugin_id="object:16c7e3c"
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:321:in `send_data'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:169:in `block in write_objects'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:163:in `times'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:163:in `write_objects'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/output.rb:459:in `write'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/buffer.rb:325:in `write_chunk'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/buffer.rb:304:in `pop'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/output.rb:320:in `try_flush'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/output.rb:140:in `run'
</span></code></pre></td></tr></table></div></figure>


<p>バッファも残っている</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ls /var/log/fluentd.test.data.*.buffer
</span><span class='line'>/var/log/fluentd.test.data.b50a3b457dcfed028.buffer  /var/log/fluentd.test.data.q50a3b455b1eac4ca.buffer</span></code></pre></td></tr></table></div></figure>


<p>しばらく放置した後、iptablesを解除したら無事に送信された.</p>

<h2>まとめ</h2>

<p>Fluentd v0.12で導入されたAt-least-once semanticsを試してみた. アプリケーションレイヤでの到達確認が実装されることで、TCPレイヤでパケットがうまく届いていないケースについても、fluentdがそれを検知して再送してくれることが確認できた.</p>

<p>ちなみに自分のところでは、ruby1.9上でfluentdを動かしていた時にプロセスが短時間ブロックするような事象が多発していて、それに起因してログのロストが発生したことがある. 恐らく、上記のようにTCPのコネクションは確立したように見えて、実は相手側がハング状態だったためにソケットバッファに滞留、最終的にソケットクローズ時にパケットが破棄されたのだと考えている.
(この時は、td-agent2にしたら解消した)</p>

<p><code>require_ack_response</code>により、そのようなケースでもfluentdがちゃんと検知して再送してくれるので、このオプションは是非入れておきたい.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dockerを使い始めるための検討]]></title>
    <link href="http://ogibayashi.github.io/blog/2014/12/10/thinkings-to-start-using-docker1/"/>
    <updated>2014-12-10T23:38:27+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2014/12/10/thinkings-to-start-using-docker1</id>
    <content type="html"><![CDATA[<p>実環境でDockerを使い始めてみよう、と思うけど考えないといけないことが色々ありそう。どんなことを考える必要があるか、それぞれどうしていくのが良いか、考えてみる。</p>

<h2>導入の目的</h2>

<p>以下を目的にする。</p>

<ul>
<li>アプリケーションのポータビリティ

<ul>
<li>開発・テスト環境と同じイメージを本番でも利用することで、環境差異に起因するトラブルを減らす</li>
<li>サーバが増えた際に、簡単に同じアプリケーション環境をセットアップできるようにする</li>
</ul>
</li>
<li>リソースの分離

<ul>
<li>同じサーバに複数のアプリケーションが乗った際に、それぞれの環境やOSリソースを分離し、お互いに影響しないようにする</li>
</ul>
</li>
</ul>


<p>導入の対象は、まずはデータをストアしないjavaアプリケーションサーバとする.</p>

<h2>考える事</h2>

<p>いざ、使ってみようと思うと以下のような点が悩ましくなってきた。</p>

<ul>
<li>イメージを作る単位</li>
<li>既存の構成管理ツール(Ansible)との連携</li>
<li>ネットワークをどうする？外部からどう接続するか</li>
<li>起動先の環境に依存する部分をどう吸収するか</li>
</ul>


<h2>考えてみる</h2>

<p>さて、どうしようか</p>

<h3>イメージを作る単位</h3>

<p>イメージの差分管理ができる、とは言え何か変わる度に一直線に更新をしていくのもなんか違う気がする.</p>

<p>以下のようにレイヤを分けて作るのが管理しやすいのでは、と考えた.</p>

<ul>
<li>baseイメージ

<ul>
<li>OSの基本パッケージと、構成管理ツールをインストールしたイメージ.</li>
</ul>
</li>
<li>ミドルウェアイメージ

<ul>
<li>baseイメージを元にビルドする. アプリケーションデプロイの手前までの環境を提供する</li>
<li>割と複雑になりそうなので、Dockerfile内からAnsible playbookを呼び出すことで実行する</li>
</ul>
</li>
<li>アプリケーションイメージ

<ul>
<li>ミドルウェアイメージに対して、アプリケーションをデプロイして使える状態にしたイメージ</li>
</ul>
</li>
</ul>


<p>新しいアプリケーションをデプロイする際は、最新のミドルウェアイメージを利用して新しいアプリケーションイメージを作成する. もしミドルウェアの設定変更などが発生した場合は、ミドルウェアイメージを再作成し、それを元に再度アプリケーションイメージを作成する.</p>

<p>それぞれのレイヤごとに更新の頻度も違うし、更新を入れる担当も違うのでこのように分けて、それぞれのレイヤでバージョンを管理していくのが良さそう.</p>

<h3>既存の構成管理ツールとの連携</h3>

<p>Ansible playbookの資産があるので、再利用したい. また、Dockerfileは基本コマンドを羅列するだけなので、ある程度複雑になってくると辛い. Dockerfileだけで環境を作ることもできるが、メンテナンス性や再利用性を考えると構成管理ツールは必要だと思う.</p>

<p>そこで、DockerfileからAnsible playbookを実行して環境を構築することにする.</p>

<p>通常ansible-playbookはSSH経由で実行するが、そのためにコンテナの中でsshdを上げるのも。。。と思ってたら、SSHを使わなくても実行できるらしい. <a href="http://docs.ansible.com/playbooks_delegation.html#local-playbooks">Local Playbooks</a></p>

<p>playbookをDockerfile内でADDするか、VOLUMEに置くかしてコンテナから見えるようにすれば、コンテナに対するplaybookの適用はできそう.</p>

<h3>ネットワークをどうする？</h3>

<p>普通にコンテナを起動すると、Dockerホスト内のプライベートネットワーク空間に入るので、外部からどう接続するかを考える必要がある.</p>

<p><a href="https://docs.docker.com/articles/networking/#container-networking">ドキュメント</a>によると、ネットワークのオプションもいくつかある.</p>

<ul>
<li><code>--net=bridge</code>(デフォルト)

<ul>
<li>Dockerホストが提供する仮想ブリッジに接続される. 外部にポートを公開するにはexposeする必要がある</li>
</ul>
</li>
<li><code>--net=container</code>

<ul>
<li>起動済みコンテナのネットワークスタックを再利用する. ネットワークスタックを共用しているコンテナ同士は、localhostで通信できる. 外部から繋ぐには、やはりexposeが必要</li>
</ul>
</li>
<li><code>--net=host</code>

<ul>
<li>ホストのネットワークスタックをそのまま利用する. 普通にホストの中にプロセスが起動しているのと同じ状態になる.</li>
</ul>
</li>
</ul>


<p><a href="http://domino.research.ibm.com/library/cyberdig.nsf/papers/0929052195DD819C85257D2300681E7B/$File/rc25482.pdf">IBMの検証結果</a>や、<a href="ISUCON%E3%81%A7Nginx%E3%81%A8MySQL%E3%82%92Docker%E5%8C%96%E3%81%97%E3%81%9F%E3%81%A8%E3%81%8D%E3%81%AE%E3%83%91%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%B3%E3%82%B9">こちら</a>によると、<code>--net=bridge</code>だとNATのオーバーヘッドにより<code>--net=host</code>に対して20%程度性能が落ちる場合があるらしい.</p>

<p>ただ、<code>--net=host</code>だと同一ホストで複数コンテナ起動する場合にはポートがバッティングしないように何かしら工夫する必要がある. <code>--net=bridge</code>だと<code>docker run</code>するときにオプションでマッピングを変えればいいので、そっちの方が扱いやすいか. これは、とりあえず両方試してみて後で決める.</p>

<p>外部からの接続方法については、<a href="https://github.com/googlecloudplatform/kubernetes">Kubernetes</a>とか、serf+HAProxy/nginxとか、カッコイイ方法はありそうだけど、とりあえずは手で接続元やHAProxy等の設定を変更することにする. 自動化は後.</p>

<h3>起動先の環境に依存する部分をどう吸収するか</h3>

<p>実行環境がコンテナ内に固められている、とは言えやはり環境に依存する部分はあり得る. アプリケーションサーバから接続する先のDBとか、ホストのメモリに応じてコンテナ内のjavaアプリケーションサーバのヒープサイズを変えたいとか、自ホストのIPアドレスが設定ファイルに書かれている場合とか.
調べてみると、<code>docker run</code>に<code>-e</code>オプションを付けて環境変数を渡すことはできそう. すると、環境変数を元に関連する部分を書き換えた上でアプリケーションサーバを起動するようなラッパースクリプトを作り、Dockerfileの<code>CMD</code>でコンテナ起動時に実行するようにする必要がある.</p>

<p>以下が参考になりそう</p>

<ul>
<li><a href="http://heartbeats.jp/hbblog/2014/07/3-tips-for-nginx-on-docker.html">nginxをdockerで動かす時のTips 3選</a></li>
<li><a href="http://java.dzone.com/articles/parameterized-docker">Parameterized Docker Containers</a></li>
</ul>


<h2>他には？</h2>

<p>ログをどうするとか、モニタリングとか、他にも色々考えることはありそうだけど、とりあえずここまで.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Stated With OpenShiftを読んだ]]></title>
    <link href="http://ogibayashi.github.io/blog/2014/09/27/getting-started-with-openshift/"/>
    <updated>2014-09-27T08:16:08+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2014/09/27/getting-started-with-openshift</id>
    <content type="html"><![CDATA[<p><a href="http://shop.oreilly.com/product/0636920033226.do">Getting Stated with OpenShift</a>を読んだので、簡単にまとめ. とりあえず4章まで.</p>

<h2>この本について</h2>

<p>OpenShiftの管理者ではなく、Webアプリケーション開発者向けの本. OpenShift Onlineを使って、どのようにWebアプリケーションを動かすことができるか、ということが書いてある</p>

<h2>1. Introduction</h2>

<ul>
<li>OpenShiftとは？

<ul>
<li>RedHatが提供するPaaS</li>
</ul>
</li>
<li>3つのバージョンがある

<ul>
<li>OpenShift Origin : オープンソースであり、最新版. 自分の環境に入れて使うことができる. OnlineやEnterpriseのUpstreamとなる.</li>
<li>OpenShift Online : RedHatが提供するクラウドサービス版. AWS上で動いており、アカウントを作ればOpenShiftの環境を使うことができる.  本書はこれを対象に書かれている.</li>
<li>OpenShift Enterprise : Productionでの利用を想定し、RedHatによりサポートされる安定版. 自分でインストールして使う.</li>
</ul>
</li>
<li>基本的な用語

<ul>
<li>Application : OpenShift上で動かすWebアプリケーション</li>
<li>Gear : サーバコンテナ. 使えるリソースに応じて、small, medium, largeの3タイプがある</li>
<li>Cartridge: Gearに追加する機能の固まり. JBossとか、Pythonとか、DatabaseとかCronとか.</li>
</ul>
</li>
</ul>


<h2>2. Creating Application</h2>

<ul>
<li>OpenShift Onlineを使うための流れは以下の通り

<ul>
<li>アカウントを作る</li>
<li>コマンドラインツール(rhc)を入れる. rhcはrubygemなので、<code>gem install rhc</code>で入れることができる</li>
<li>コマンドラインツールをセットアップする. <code>rhc setup</code>で行う. OpenShiftのAPIをコマンドラインから使うための認証情報などを登録する.</li>
</ul>
</li>
<li>以後、OpenShift上のアプリケーション管理は<code>rhc</code>コマンドで行う</li>
<li>OpenShiftのアプリケーションを作るには<code>rhc app create insultapp python-2.7</code>のようにする. 引数は、アプリケーション名と言語.

<ul>
<li>実行すると、アプリケーションにアクセスするためのURLや、GearにSSHログインするためのユーザ名@ホスト名が表示される</li>
<li>また、カレントディレクトリにGitリポジトリが作成される. このリポジトリ内にアプリケーションのコードや色々な設定が格納される.</li>
</ul>
</li>
<li>アプリケーションを作成する際に、 <code>-g</code>オプションを付けるとautoscaling機能が有効になる. 有効にすると、HAProxyがセットアップされ、アプリケーションがスケールアウトできるようになる

<ul>
<li>本書では簡略化のためautoscalingは使っていないが、有効にしておいた方が良い</li>
</ul>
</li>
<li>smallのgearであれば無償で使える. 無償版の場合、48時間リクエストが無いとアプリケーションは一旦ディスクに書かれ、次回リクエスト時に再ロードされる (応答に時間がかかる)

<ul>
<li>お金を払えば、gearを増やしたり、より大きなgearを使ったり、ディスク容量を拡張したり、JBoss EAP等の有償Cartridgeを使ったり、サポートチケットを発行したり、などなどができる.</li>
</ul>
</li>
</ul>


<h2>3. Making Code Modifications</h2>

<ul>
<li>アプリケーション作成時に作成されたGitリポジトリ内にコードを書いて、<code>git push</code>するとデプロイされる</li>
<li><code>.openshift/action_hooks</code>ディレクトリ内にファイルを作成しておくことで、デプロイの特定のタイミングで任意の処理を実行することができる. (action hook script)</li>
<li>通常、アプリケーションのデプロイ時にアプリケーションは一旦停止する. <code>.openshift/markers/hot_deploy</code> ファイルを作成しておくと、ホットデプロイが有効になる.</li>
</ul>


<h2>4. Adding Application Components</h2>

<ul>
<li>Cartridgeを追加することで、アプリケーションに様々な機能を追加することができる(<code>rhc cartridge add &lt;cartridge名&gt;</code>). この章では以下のCartridgeが紹介されている.</li>
<li>Database

<ul>
<li>PostgreSQL, MySQL, MongoDB等のデータベースを使うことができる</li>
<li>アプリケーションがscalableでなければ同じgearに、そうでなければDatabase専用gearが作成され、その中にインストールされる</li>
<li>追加時に、DBへアクセスするためのアカウントやURL等が表示される</li>
</ul>
</li>
<li>Cron

<ul>
<li>cronを使うことができる</li>
<li>git repositoryの<code>openshift/cron/{minutely,hourly,daily,weekly,monthly}/</code>以下にファイルを作成し、<code>git push</code>すると有効になる</li>
</ul>
</li>
<li>Continuous Integration (jenkins)

<ul>
<li>まず、<code>rhc app create</code>でJenkins用のアプリケーションを作成する</li>
<li>その上で、元のアプリケーションに<code>rhc cartridge add</code>でJenkinsのクライアントをCartridgeとして追加.</li>
<li>これにより、アプリケーションをpushした際にJenkins上でビルドやテストが走るようになる. ビルドが失敗した場合、アプリケーションはデプロイされない</li>
</ul>
</li>
<li>Metrics and Monitoring

<ul>
<li><code>rhc cartridge add metrics-0.1 -a appname</code>のようにすることで、アプリケーションのモニタリング画面が追加される. CPUやメモリの状況等を確認可能</li>
</ul>
</li>
<li>その他にも、コミュニティで開発された多数のCartridgeが存在し、利用することができる.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[第6回Elasticsearch勉強会]]></title>
    <link href="http://ogibayashi.github.io/blog/2014/09/17/elasticsearch-study-6th/"/>
    <updated>2014-09-17T00:19:06+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2014/09/17/elasticsearch-study-6th</id>
    <content type="html"><![CDATA[<p>第6回Elasticsearch勉強会に参加したので、記憶が新しいうちにメモ. 内容は資料を見れば分かると思うので、個人的に特に記憶に残ったポイントと、感想のみ.</p>

<h2>Aggregationあれこれ @johtani 氏</h2>

<ul>
<li>ElasticsearchのAggregation機能を使うと、SQLで言う集計 group by 的なものができると、という話</li>
<li>動きとしては、各shard内でaggregateし、最後にその結果を集約する、という形. 普通のqueryと変わらない</li>
<li>クエリの種類によっては、正確な値ではなく近似値だったりする</li>
<li>Field Collapsingという、URLごとにtop Nを検索するようなこともできる</li>
<li>個人的な感想

<ul>
<li>便利な機能なので、是非使いたい. 現状だとKibana3で使えないのがネックだけど、Kibana4だと使えるようになるらしいので期待</li>
</ul>
</li>
</ul>


<h2>秒間3万の広告配信ログをElasticSearchで リアルタイム集計してきた戦いの記録 @satully 氏</h2>

<ul>
<li><a href="http://www.slideshare.net/Satully/elasticsearch-study6threaltime20140916">資料</a></li>
<li>商用での利用事例、という意味ですごく興味深かった</li>
<li>DSPの配信システムで利用. 最大秒間30000ログ. 1.5TB/日くらい</li>
<li>各サーバのログ &ndash;> fluentd &ndash;> Elasticsearch &ndash;> MySQL/Redis という構成</li>
<li>fluentdは10インスタンス</li>
<li>ElasticsearchはCoordinate:2ノード, Search:2ノード, Data:28ノード.全てAWSのr3.largeインスタンス. メモリは30GBくらい？

<ul>
<li>diskは途中でSSDに変えた</li>
<li>CoordinateとSearchを分離する、というのは世間で見かけるけど、実際に負荷をみると必要性が疑問</li>
<li>1日1index, 1index12shard, 1レプリ.</li>
</ul>
</li>
<li>単純なログのinsertのみではなく、Bidと紐付けるべきログが来ると、元のBidのログに項目を追加する. fluentd pluginとして実装</li>
<li>運用ツールとしては、Elasticsearchのpluginとしてhead, bigdesk, ElasticHQ. 死活監視やfluentdのメトリックにはZABBIX</li>
<li>個人的な感想

<ul>
<li>サーバスペック、台数、流量の具体的な事例としてとても参考になった</li>
<li>indexサイズはかなり大きくなるけど、いけるものなんだ.</li>
<li>お金の計算に関わる部分に、fluentdとかElasticsearchとか使っている、というのに驚いた</li>
</ul>
</li>
</ul>


<h2>Elasticsearch 日本語スキーマレス環境構築と、ついでに多言語対応 @9215 氏</h2>

<ul>
<li><a href="https://speakerdeck.com/kunihikokido/elasticsearch-ri-ben-yu-sukimaresuhuan-jing-gou-zhu-to-tuideniduo-yan-yu-dui-ying">資料</a></li>
<li>スキーマをちゃんと設計して、それに合わせてマッピング定義するのは面倒. どんなフィールドでどんなマッピングが最適、とか個別に設計すると人材がスケールしない.</li>
<li>そこで、dynamic templateとindex templateを使い、ノウハウが必要な部分はtemplateに集約、各利用者(データを投入する人)にはフィールド名のネーミングルールを周知する、というアプローチにした</li>
<li>例えば、利用する言語を&#8221;language&#8221;というフィールドの値として設定し、template側でそれに合わせて適切なanalyzerを定義しておくことで、多言語に対してもうまくindexできる.</li>
<li>個人的な感想

<ul>
<li>確かに、どんな型にして、どんなanalyzerにして、とかはノウハウの部分になるので、それをtemplateに集約するのはとても良いアプローチだと思った</li>
</ul>
</li>
</ul>


<h2>elasticsearchソースコードを読みはじめてみた @furandon_pig 氏</h2>

<ul>
<li>Elasticsearchのソースコードを読み始めてみた</li>
<li>開発者の人には、はじめにREST APIを受けて検索する部分の動作をみてみるといいよ、と言われたけど、それがどこか分からなかったので、起動の部分を追いかけてみた話</li>
<li>個人的な感想

<ul>
<li>ちょうど、そろそろソースみないと、と思っていたのでそのとっかかりとして大変参考になった</li>
</ul>
</li>
</ul>


<h2>(LT)Elasticsearch RerouteAPIを使ったシャード配置の制御 @pisatoshi 氏</h2>

<ul>
<li><a href="https://speakerdeck.com/pisatoshi/elasticsearch-rerouteapiwoshi-tutasiyadopei-zhi-falsezhi-yu">資料</a></li>
<li>Reroute APIを使って、具体的にどのシャードをどこに配置するとかの制御ができるよ、という話</li>
<li>リバランスを有効にしていると、APIでシャードを別のノードに配置しても、リバランスされてしまうので注意</li>
<li>個人的な感想

<ul>
<li>オチがとてもおもしろかった.</li>
<li>Reroute APIって、制御できるのは良いけど、逆にいちいち制御するのはやってられないし、どーいう場面で使うのだろう、と思った.</li>
</ul>
</li>
</ul>


<h2>(LT)検索のダウンタイム0でバックアップからIndexをリストアする方法 @k_bigwheel 氏</h2>

<ul>
<li><a href="http://www.slideshare.net/kbigwheel/0index-39143333">資料</a></li>
<li>indexのスナップショットを取れるようになったけど、普通にやるとリストア対象のindexを一旦closeしないといけない. サービスの継続を考えると、ダウンタイム無しでやりたい、という話</li>
<li>予めクライアントからはalias経由でアクセスするようにしておく. aliasの切り替えはatomicにできるので、それを利用して、リストア完了後にスッパリ切り替える、という形にする</li>
<li>個人的な感想

<ul>
<li>リストアだけでなく、mappingを変えるためにindexしなおすとか、aliasを使うと便利な場面はありそう</li>
<li>しかし、自分の環境はfluentdで&#8221;name-YYYY.MM.DD&#8221;の形でindexを作っているので、この場合aliasはどうしたらいいんだろう</li>
</ul>
</li>
</ul>


<h2>全体所感</h2>

<ul>
<li>最近、Elasticsearchを真面目に使い始めてみたので、どのトークも大変参考になった. 特に、 @satully 氏の話は良かった</li>
<li>結構、みんなAWSなんだな</li>
<li>懇親会で、発表についてのもう少し詳しい話とか、他の人がどんな感じに使ってるとか聞けたので、そっちも良かった. しかも参加費無料. 素晴らしい.</li>
<li>主催の@johtaniさん、会場提供のリクルートテクノロジーズさんに感謝.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentdの障害時動作]]></title>
    <link href="http://ogibayashi.github.io/blog/2014/06/04/how-fluentd-works-in-case-of-failures/"/>
    <updated>2014-06-04T23:12:23+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2014/06/04/how-fluentd-works-in-case-of-failures</id>
    <content type="html"><![CDATA[<p>Fluentdが障害の時にどのような動作をするのか調べてみたので、そのメモ. td-agent 1.1.17(fluentd v0.10.39)で確認したつもりだが、もしかしたらもう少し新しいので確認したケースもあるかも. BufferedOutputを中心に記載している.</p>

<h2>BufferdOutputの基本</h2>

<p>fluentdの特徴の一つとして、fluentd送信先で障害があり、メッセージが送れなかった場合は大抵(BufferedOutputを使っているプラグインであれば)fluentdでバッファリングし、一定時間後に再送してくれる.</p>

<p>このバッファリングのサイズは、BufferedOutputプラグインのbuffer_chunk_limit*buffer_queue_limitで決まる.</p>

<p>これらのデフォルト値は以下に解説付きでまとまっている. (良く参照させて頂いています)
<a href="http://d.hatena.ne.jp/tagomoris/20130123/1358929254">FluentdでバッファつきOutputPluginを使うときのデフォルト値</a></p>

<h2>何回くらいリトライするの？ リトライの間隔は？</h2>

<p>リトライの回数は、retry_limit(デフォルト 17)で指定された回数まで. 間隔は一定ではなく、段々延びていく. 具体的に間隔を計算してるのは、BufferedOutput#calc_retry_wait.</p>

<p>リトライ間隔は、以下のパラメータでコントロールすることができる</p>

<ul>
<li>max_retry_wait(デフォルト: nil = 上限なし)</li>
<li>retry_wait (デフォルト: 1.0)</li>
</ul>


<p>同じメソッドを使って、実際にどれくらいになるのかを計算させてみた. 以下で例えば、1=>2は、一度送信に失敗してから、2回目の送信を試みるまでという意味. 単位は秒. 全てデフォルト値だと、以下の様な感じ. 最大だと、33765秒=9時間22分になる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1=&gt;2 : 1.0799124443510373
</span><span class='line'>2=&gt;3 : 1.8141315928054327
</span><span class='line'>3=&gt;4 : 3.5115188260172046
</span><span class='line'>4=&gt;5 : 7.106397160810471
</span><span class='line'>5=&gt;6 : 14.175590112052593
</span><span class='line'>6=&gt;7 : 31.434005639868758
</span><span class='line'>7=&gt;8 : 68.4743252224448
</span><span class='line'>8=&gt;9 : 116.47949944913451
</span><span class='line'>9=&gt;10 : 279.97276701667636
</span><span class='line'>10=&gt;11 : 487.69976826480445
</span><span class='line'>11=&gt;12 : 909.7729519328531
</span><span class='line'>12=&gt;13 : 2125.0559803853725
</span><span class='line'>13=&gt;14 : 3717.0255349933364
</span><span class='line'>14=&gt;15 : 8658.913465429461
</span><span class='line'>15=&gt;16 : 18189.354025481873
</span><span class='line'>16=&gt;17 : 33765.98470398931
</span></code></pre></td></tr></table></div></figure>


<p>例えば、max_retry_wait=120とすると、以下のようになる. 何回リトライしても、リトライ間隔の上限はmax_retry_waitまでになる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1=&gt;2 : 1.0717022666140232
</span><span class='line'>2=&gt;3 : 1.9866738239982864
</span><span class='line'>3=&gt;4 : 3.9258714996769903
</span><span class='line'>4=&gt;5 : 7.002702902759963
</span><span class='line'>5=&gt;6 : 15.817343449261045
</span><span class='line'>6=&gt;7 : 34.49173945537066
</span><span class='line'>7=&gt;8 : 65.98469012616731
</span><span class='line'>8=&gt;9 : 120
</span><span class='line'>9=&gt;10 : 120
</span><span class='line'>10=&gt;11 : 120
</span><span class='line'>11=&gt;12 : 120
</span><span class='line'>12=&gt;13 : 120
</span><span class='line'>13=&gt;14 : 120
</span><span class='line'>14=&gt;15 : 120
</span><span class='line'>15=&gt;16 : 120
</span><span class='line'>16=&gt;17 : 120
</span></code></pre></td></tr></table></div></figure>


<p>retry_waitを半分の0.5にすると、全てのリトライ間隔が半分になる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1=&gt;2 : 0.46442639898905974
</span><span class='line'>2=&gt;3 : 0.9688421553729557
</span><span class='line'>3=&gt;4 : 2.2291735347851613
</span><span class='line'>4=&gt;5 : 3.545406346443683
</span><span class='line'>5=&gt;6 : 7.824124603156501
</span><span class='line'>6=&gt;7 : 17.564462446502926
</span><span class='line'>7=&gt;8 : 30.97024814321994
</span><span class='line'>8=&gt;9 : 71.84343582620227
</span><span class='line'>9=&gt;10 : 127.87010583643446
</span><span class='line'>10=&gt;11 : 286.751861977861
</span><span class='line'>11=&gt;12 : 551.32668884554
</span><span class='line'>12=&gt;13 : 1077.2785515357239
</span><span class='line'>13=&gt;14 : 2095.196745718026
</span><span class='line'>14=&gt;15 : 3995.080966184667
</span><span class='line'>15=&gt;16 : 9131.408473518048
</span><span class='line'>16=&gt;17 : 16810.484835714517
</span></code></pre></td></tr></table></div></figure>


<p>リトライの頻度を増やす(リトライ間隔を減らす)場合は、併せてretry_limitも変更しないと、早々にリトライアウトしてしまう、ということになるので注意.</p>

<h2>リトライ回数が超過したら？</h2>

<p>リトライ回数を超過した場合、secodaryディレクティブを指定しておけば、そちらに出力される. 通常は、ファイルに出力しておいて、後からリカバリに使う、というケースが多いと思う.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;secondary&gt;
</span><span class='line'>    type file
</span><span class='line'>    path /path/to/forward-failed
</span><span class='line'>  &lt;/secondary&gt;</span></code></pre></td></tr></table></div></figure>


<p>このようなケースでは、ログに以下のように出力される</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-06-22 07:06:40 +0900 [warn]: fluent/output.rb:352:rescue in try_flush: retry count exceededs limit. falling back to secondary output.
</span></code></pre></td></tr></table></div></figure>


<h2>キューが溢れたら？</h2>

<p>キュー(バッファ)が溢れると、fluentdのログに以下のようなメッセージが出る.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-05-25 11:30:23 +0900 [warn]: fluent/engine.rb:149:rescue in emit_stream: emit transaction failed  error_class=Fluent::BufferQueueLimitError error=#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;
</span></code></pre></td></tr></table></div></figure>


<p>この場合、inputプラグインがEngine.emitを実行する際にExceptionが発生する. プラグインが、これをrescueしていない場合、inputプラグインは停止する. rescueしている場合はinputプラグインの実装次第だが、大抵Exceptionは無視されてemitしたデータは破棄される.
(既に溜めるためのキューがあふれているので、それしか無い)</p>

<h2>送信先が復活したら?</h2>

<p>再送中に送信先が復活し、再送に成功した場合は以下の様なメッセージが出力される.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-05-25 11:33:01 +0900 [warn]: fluent/output.rb:312:try_flush: retry succeeded. instance=70365422937420</span></code></pre></td></tr></table></div></figure>


<p>ここで、注意点として送信先が復活してもすぐに再送してくれるわけではない. これは、送信先とのハートビートを行っているout_forwardでも一緒. BufferedOutput#try_flushのコードを見ると分かるが、リトライ中で、まだ次のリトライ時刻に達していない場合は、送信は行わない.</p>

<p>なので、retryを繰り返して再送間隔が延びている場合は、次の再送タイミングになるまでキューが溜まり続ける(もしくは、既に溢れている場合は溢れ続ける)</p>

<h2>キューを強制的に送信することはできないの？</h2>

<p>リトライ中の場合以外であれば、fluentdのプロセスにSIGUSR1を送ることでキューが吐き出される. リトライ中の場合は、次のリトライタイミングまでは送信されない. 全てのキューを吐き出すには、プロセスを停止するしかない.</p>

<p>プロセス停止時の挙動は使用しているバッファプラグインによって異なるが</p>

<ul>
<li>buf_memoryの場合

<ul>
<li>プロセス停止時に全てのキューが吐き出される.</li>
</ul>
</li>
<li>buf_fileの場合

<ul>
<li>flush_at_shutdownがtrue(デフォルト false)の場合のみ、プロセス停止時に全てのキューが吐き出される.</li>
</ul>
</li>
</ul>


<p>長くなったのでここまで.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppetを使っていて思ったことなどをまとめてみる]]></title>
    <link href="http://ogibayashi.github.io/blog/2014/02/26/about-puppet-1/"/>
    <updated>2014-02-26T22:45:06+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2014/02/26/about-puppet-1</id>
    <content type="html"><![CDATA[<h2>この記事について</h2>

<p>puppetを初めて触ってから3,4年くらい経ったが、その間に思ったことなどをまとめてみる. なぜpuppetか、何に使っているか、使っていて思ったこと、課題など.</p>

<h2>利用状況を簡単に</h2>

<p>こんな環境で使ってます.</p>

<h3>puppetの使用環境</h3>

<p>CentOS 5 and 6で、puppetは2.7.21. 大体4つくらいの独立した(ネットワーク的に分断された)環境で、サーバ台数は合計百数十台. サーバ上で稼働する(した)ソフトウェアはHadoop, fluentd, MongoDB, Cassandra, MySQL, Munin等.</p>

<h3>何に使っているか</h3>

<p>上記ソフトウェアの環境は基本的にpuppetで構築している.が、puppetで完結できていることだけではないので、それは後述</p>

<h2>使っていて思ったこと</h2>

<p>今まで使っていて思ったことなどを書いてみる.</p>

<h3>puppetで何が良いか</h3>

<p>puppetだけではなく、chef等でも一緒だと思うけど、この手のソフトウェアを使わない場合だと、シェルスクリプトで頑張るか、VMのイメージをコピーするか、になると思う. それぞれに対して比較すると</p>

<ul>
<li>シェルスクリプト等でやる場合

<ul>
<li>色々全部自分で書かないといけないのでしんどい. 単一の環境、初回構築なら良いけど、ちょっと違う環境を作るとか、一旦作った環境に変更を加えるとか. 規模が大きくなってくると、モジュール化したり、過去に作ったものを再利用したくなるけど、その点でもつらい.</li>
</ul>
</li>
<li>VMのイメージコピーの場合

<ul>
<li>既に構築済の環境に対する変更をどう適用するか、環境の変更履歴をどう管理するか、というところが課題.</li>
</ul>
</li>
</ul>


<p>puppetは、&#8221;このサーバはこうあるべき&#8221;と定義すれば、前の状態がどうであってもその状態にしてくれるので、いちいちチェックして、期待と違ったら処理をして、みたいなことを書かなくて済む. また、クラス、モジュール、等再利用性を促すような言語の機能もあるので、うまく書けば既存のコードを流用しつつ、新しい要件に対応するようにマニフェストを変更していくようなこともやりやすい.</p>

<p>ただ、実はそれだけではなくてInfrastructure as Code, サーバ環境がコードで表現できるようになり、アプリケーションのコードと同じように変更管理、レビュー、テスト、デプロイができるようになるのが大きいと思う. このあたりは<a href="http://d.hatena.ne.jp/naoya/20131215/1387090668">naoyaさんの記事</a>に詳しく書かれている.</p>

<h3>puppetの使いどころ</h3>

<p>サーバ構築のうち、何をpuppetでやるべきか？サーバ構築はBootstrapping, Configuration, Orchestrationの3つのレイヤに分けて考えられることが多いが、この中だとConfigurationの部分をpuppetでやることが有効</p>

<p>つまり、</p>

<ul>
<li>Bootstrappingに相当する部分、OSインストール、ネットワーク設定、puppet自体のインストール、はやらない(やれない)</li>
<li>Orchestrationに相当する部分、複数台の協調操作が必要な操作もやらない. 今はここはfabric(局所的にserf)でやっている.</li>
<li>その他、puppetが動作可能になった後のconfigurationで、単一サーバ内で完結するようなものは全てpuppetで行う.</li>
</ul>


<p>という感じにしている.</p>

<h3>マニフェストのいい感じの書き方</h3>

<p>いい感じ、というのは読みやすく、再利用しやすい、ということ. これは、使い始めてからずっと試行錯誤しているところ. そのための要素としては、以下があると思う.</p>

<ol>
<li>モジュールの書き方</li>
<li>モジュールを組み合わせて、実際のサーバに適用するマニフェストにする部分をどう書くか</li>
<li>環境依存する部分をどのように切り出すか</li>
</ol>


<p>まず、1について. モジュールはライブラリのように環境が変わっても再利用可能なものであるべき. どのような書き方が良いか、というのは<a href="http://docs.puppetlabs.com/guides/module_guides/bgtm.html">このドキュメント</a>に書かれている. puppetlabs公式のntpモジュールが、お手本としては良いらしい.</p>

<p>2については、以下で紹介されている、RoleとProfileという考え方を導入するのが良い.</p>

<ul>
<li><a href="http://www.slideshare.net/PuppetLabs/roles-talk">http://www.slideshare.net/PuppetLabs/roles-talk</a></li>
<li><a href="http://www.craigdunn.org/2012/05/239/">http://www.craigdunn.org/2012/05/239/</a></li>
</ul>


<p>moduleを複数まとめてprofileが構成され、さらに複数profileをまとめてroleが構成される. node(=物理ホスト)は、一つのroleに紐付けられる、という形になっている. 例えば、<code>profile::webserver</code>にはhttpdとphpが必要で、<code>role::www::dev</code>は、<code>profile::webserver</code>と<code>profile::database</code>が含まれる、みたいな形に書く. ただ、自分はまだroleの必要性がしっくりきてないので、profileとroleを分けていない.</p>

<p>3については、hieraやENCを使ってあるサーバに適用するクラス(role/profile)や、そのパラメータをマニフェストの外に切り出す、というのが定番らしい. ただ、自分はまだこれらは導入していなくて、以下のように変数をまとめたクラスを作って、それを呼び出すときの引数で環境を切り替える、という風にしている. 今後はhiera or ENCに移行していきたい.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class base_env($envname="") {
</span><span class='line'>  case $envname {
</span><span class='line'>    "testenv" :{
</span><span class='line'>      $reposrv = "192.168.1.1"
</span><span class='line'>      $repourl = "http://$reposrv/Lang/Ruby/"
</span><span class='line'>    }
</span><span class='line'>    default: {
</span><span class='line'>      $masternode="192.168.1.2"
</span><span class='line'>      $reposrv = "example.deploy.local"
</span><span class='line'>      $repourl = "http://$reposrv/Lang/Ruby/"
</span><span class='line'>      $gfurl="http://$masternode:5125"
</span><span class='line'>      $munin_node=$masternode
</span><span class='line'>      $serf_join = ['192.168.1.1','192.168.1.2']
</span><span class='line'>      $munin_allow='^192\.168\..*$'
</span><span class='line'>
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class profile::base($envname="") {
</span><span class='line'>  class{"base_env":
</span><span class='line'>    envname =&gt; $envname
</span><span class='line'>  }
</span><span class='line'>  ...
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>node /hadooptest\.hadoop.local/ {
</span><span class='line'>   class{"profile::base":
</span><span class='line'>     envname =&gt; "testenv"
</span><span class='line'>   }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>puppetmasterについて</h3>

<p>つい最近まではpuppetmasterを使っていた. ただ、以下の問題があった.</p>

<ul>
<li>新しくサーバを構築するときにpuppetmasterが無ければ立てる必要がある</li>
<li>証明書が面倒. 自動署名にはしてるけど、同じホスト名でサーバの再構築することもそれなりにあったので、その度に証明書のクリアが必要. クリアしてなくてうまく同期できない、というのも良くあった.</li>
</ul>


<p>なので、最近はgitリポジトリにmanifestを配置して、各サーバでgit clone. という風にしている. マニフェストを更新した時は、fabricを使って並列にgit pull &amp; puppet applyする. 証明書の問題から開放されたし、サーバ台数が増えてもgitリポジトリを複数にすればスケールするし、gitが無くてもmanifestだけコピーすれば環境作れるし、でとても楽になった.</p>

<h2>今後の課題</h2>

<p>今はできていないので、これから取り組みたいこと.</p>

<h3>テスト</h3>

<p>上記の通り、puppet化、というかコード化するメリットとしてCI的なものは外せないと思うのだけど、まだできていない.
幸い<a href="http://blog.tmtk.net/2013/09/28/docker-jenkins-serverspec-puppet.ja.html">先人の事例</a>があるので、こちらを真似しながら環境はできた. ただ、まだ実験的な段階で、プロセスとして固まってない. Dockerだけでなく実機の環境もあるので、Dockerでテストして、通ったものは即検証環境に自動的にデプロイして、みたいな形にしたい.</p>

<h3>外部モジュール(Puppet Forge)の利用</h3>

<p>puppetを始めた当初は、公開されているモジュールがあると言っても今一つやりたいことにフィットしなくて、全て自分で書いていたのだけど、最近は公開されているものを有効活用した方が良い気がしてきた. 実際serfとかsensuとかのモジュールを使ってみたが、全然変更の必要が無く使えたので、これからは外部のものも使っていきたい. ただ、自分のいる環境はInternetにつながらないので<code>puppet module</code>コマンドは使えず、どうやって配布するかを考えないといけない. (自前リポジトリとか立てられるのかな&hellip;)</p>

<h2>最後に</h2>

<p>最近Infrastructure as Codeという言葉も良く聞かれるようになったし、自分も今までpuppetを使ってて思ったことを一度まとめてみようと思って書いてみた. しかし、puppetってやっぱり日本語の記事が少ない. 入門的な記事はあるのだけど、どう使ってるとか、何に困ったとか、どう工夫してるとかの情報がもっとあると良いなぁ. Qiitaでそれぞれのタグを見てみたら、Chefの投稿が242に対して、Puppetは11でした&hellip;</p>
]]></content>
  </entry>
  
</feed>
