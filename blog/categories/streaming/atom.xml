<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Streaming | Tech Notes]]></title>
  <link href="http://ogibayashi.github.io/blog/categories/streaming/atom.xml" rel="self"/>
  <link href="http://ogibayashi.github.io/"/>
  <updated>2016-10-22T07:42:43+09:00</updated>
  <id>http://ogibayashi.github.io/</id>
  <author>
    <name><![CDATA[OGIBAYASHI Hironori]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[現在稼働しているFlinkクラスタについて]]></title>
    <link href="http://ogibayashi.github.io/blog/2016/10/21/about-my-flink-deployment/"/>
    <updated>2016-10-21T22:35:50+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2016/10/21/about-my-flink-deployment</id>
    <content type="html"><![CDATA[<p>先日の<a href="http://www.slideshare.net/linecorp/b-6-new-stream-processing-platformwith-apache-flink">発表</a>で、Apache Flinkを導入するに至った経緯を話したのだけど、具体的な構成とかには触れられなかったので書いておく。</p>

<h2>クラスタの構成について</h2>

<p>今運用してるFlinkクラスタは２つ。サービスで使うためのデータを生成しているものと、社内のレポーティングやモニタリングで使っているもの。前者の方は安定性重視、後者は割とカジュアルにジョブを追加したり、構成を弄ったりできるもの、という感じになっている.</p>

<p>Flinkとしては、クラスタのデプロイメント方式として、独立したdaemonとして動かす方法と、YARNの上で動かす方法があるのだけど、前者の方法にしている. その方が運用上もわかりやすいし、レイヤが少ない分トラブルも少ないだろう、というのが理由.</p>

<p>どちらも物理サーバで、TaskManagerサーバは前者が3台、後者が10台になっている. Flinkのバージョンはそれぞれ1.0.3と1.1.1. JobManagerはもちろんHAで、Flinkが使うHDFSやZookeeperは既存Hadoopクラスタを共用している.</p>

<h2>周辺の構成</h2>

<p>Flinkへのインプットはfluentdで集めたログをKafkaに投入したもの。もともとログはがっつりfluentdで集めてたので、Kafkaを導入して、そちらにも飛ばすようにした。ちなみに、fluentd→kafkaの部分について、導入当初はfluent-plugin-kafkaが使っていたposeidon gemがメンテナンス停止を宣言したりしてて若干不安があったけど、その後poseidonはruby-kafkaに置き換えられ、pluginもfluentdの公式になったりして、安心感が大分増した。開発者の方々には感謝してます。</p>

<p>アウトプットはRedis, Elasticsearch, Kafkaの3パターンがある. 汎用的に使いやすいのはElasticserachで、Kibanaで見たり、集計結果をAPIサーバ経由で他に提供したりしている. Redisは最新の情報にしか興味がなくて、かつ更新頻度が高い場合に使っている. 特定のキーの値を10秒間隔でアップデート、とか. Kafkaは、集計結果を他と連携したいケース. 今は、<a href="https://github.com/ogibayashi/kafka-topic-exporter">kafka-topic-exporter</a> 経由で集計結果を<a href="https://prometheus.io/">Prometheus</a>に入れ、<a href="http://grafana.org/">Grafana</a>で見るために使っている.</p>

<h2>運用周り</h2>

<p>モニタリングは基本的にPrometheus. <a href="https://github.com/matsumana/flink_exporter">flink-exporter</a>と<a href="https://github.com/prometheus/jmx_exporter">jmx-expoter</a>を使ってメトリクスをPrometheusに送り、Grafanaで見ている.</p>

<p>Flinkのメトリクスについては <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/metrics.html">https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/metrics.html</a> に記載があるが、JMXで見るためには、flink-conf.yamlに以下の設定を追加する. なお、これが使えるのはFlink 1.1以降.</p>

<p><code>
metrics.reporters: jmx_reporter
metrics.reporter.jmx_reporter.class: org.apache.flink.metrics.jmx.JMXReporter
env.java.opts: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=5560
</code></p>

<p>jmx_expoterの設定は、<a href="https://gist.github.com/ogibayashi/26c976442592b285fd0ff6b07b08ec60">こんな感じ</a> のものを使っている</p>

<p>すると、こんな感じでFlinkのKafkaConsumerのlagやconsumeされているレコード数を見ることができる.</p>

<p><img src="/images/2016/grafana_flink_kafka.png" alt="Flink_Grafana_Kafka" /></p>

<p>以下は、あるジョブのチェックポイントのサイズ. 長期のスパンで見ると、チェックポイントサイズが増加傾向なので何かしら不要なstateがpurgeされずに溜まっていることが疑われる</p>

<p><img src="/images/2016/grafana_flink_checkpoint.png" alt="Flink_Grafana_Checkpoint" /></p>

<p>監視は外部からdaemonのTCPポートが空いていることを確認している. JobManagerはWebUIポート(8081)を見れば良いが、TaskManagerのポートは起動するたびに変わるので、以下のような設定を入れて固定している.</p>

<p><code>
taskmanager.rpc.port: 6122
taskmanager.data.port: 6121
</code></p>

<p>Flink的なメトリクスとしては、Exceptionの発生数とか、ジョブが失敗や再起動状態にあるジョブの数とかも使ってアラートを上げた方がいいんだろうなぁ、と思いつつまだやっていない</p>

<h2>まとめ</h2>

<p>今productionで動かしているFlinkクラスタについて、構成や運用周りを紹介してみた. これからFlinkを使ってみよう、という人の参考になれば幸いです.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Realtime Data Processing at Facebookを読んで]]></title>
    <link href="http://ogibayashi.github.io/blog/2016/07/12/realtime-data-processing-at-facebook/"/>
    <updated>2016-07-12T23:16:10+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2016/07/12/realtime-data-processing-at-facebook</id>
    <content type="html"><![CDATA[<p>てすと</p>

<p><a href="https://research.facebook.com/publications/realtime-data-processing-at-facebook/">https://research.facebook.com/publications/realtime-data-processing-at-facebook/</a> を読んだのでなんとなくまとめつつ思ったことなどを書いてみる.
原文のまとめというよりは、主観と自分の理解の記述を多めにしているので、興味を持った人は是非原文を読むことをお勧めします.</p>

<h2>この文章は何か</h2>

<p>Facebookでストリーム処理を運用してきた経験を元に、ストリーム処理として重要な要件、設計上のポイント、Facebookで実際にどうしているか、と言ったことが書いてある. 個人的には、ストリーム処理というものを、製品の比較とかではなくてその機能/非機能面から紐解いてる感じで参考になった.</p>

<h2>ストリーム処理で考慮すべき要件</h2>

<p>ストリーム処理の要件を定義する上では以下が重要</p>

<ul>
<li>Ease of use

<ul>
<li>どれくらい複雑なことをやりたいか. SQLレベルか、プログラムが必要なレベルか.</li>
</ul>
</li>
<li>Performance

<ul>
<li>要求されるレイテンシとスループット</li>
</ul>
</li>
<li>Fault-tolerance

<ul>
<li>対障害性はどの程度必要か？障害時のデータ重複や欠損は許容されるか</li>
</ul>
</li>
<li>Scalability

<ul>
<li>データ流量の変化にどれだけ柔軟に対応できるか</li>
</ul>
</li>
<li>Correctness

<ul>
<li>ACIDが必要？ 入力されたデータは全て確実に出力に反映されている必要があるか</li>
</ul>
</li>
</ul>


<p>例えば、カジュアルなストリーム処理の例としてfluentd+Norikraを考えてみる. これはとても簡単に使えるし、10,000msg/sec程度なら大体問題なく処理できるけど、それ以上にスケールしたいとか、障害時含めてデータの欠損・重複は許さんとか言われると、それは違うソリューションを持ってこないと無理だよね、となる. なので、この辺の観点から要件を明確にするのは重要、と思う.</p>

<h2>Facebookのシステム</h2>

<p>大きく分けると、メッセージバス、ストリーム処理、データストアの3機能になる.</p>

<ul>
<li>メッセージバス

<ul>
<li>scribe. scribeってfluentd的な感じでログを転送・集約してるのかと思ったら、やってることはKafkaらしい. クライアントは自由にcategory(Kafkaで言うtopic)をsubscribeできるし、データは永続化されていてリプレイもできる</li>
</ul>
</li>
<li>ストリーム処理

<ul>
<li>Puma, Stylus, Swiftと3つある.</li>
<li>PumaはSQLで簡単に使える、Swiftはscribeのストリームのここまで読んだよ、というチェックポイント機能を提供する、Stylusは高機能で色々できるけど、C++でコードを書くので処理の記述はちょっと大変、と理解した. Swiftがちょっとよくわからない.</li>
</ul>
</li>
<li>データストア

<ul>
<li>Laser, Scuba, Hiveの3つ</li>
<li>Laserはストリーム処理からも使える高スループットKVS. RocksDBベース.</li>
<li>Scubaはアドホッククエリ用のCube.</li>
<li>Hiveはとにかく全てのデータが入っているDWH. クエリにはPrestoが使える</li>
</ul>
</li>
</ul>


<p>lessons learnedの章にもあるけど、ストリーム処理に複数のプロダクトがあるというのは結構重要で、Pumaでカジュアルに始めつつ、効果が出そうでもっと色々やりたくなったらStylusを使う、というようなことができる.
ストリーム処理とデータストアの関連で言うと、Laserは高スループットをさばけるのでストリーム処理中で使ったり、ストリーム処理の出力を入れたりする、Scubaはストリーム処理で加工した結果を入れる、という感じらしい.</p>

<h2>Design Desicions</h2>

<p>ここが、この文章で最も重要なところ. ストリーム処理を設計する上でのポイント、そしてFacebookではどのような選択をしたか、が書かれている. どれも正解があるわけではなく、複数の選択肢から要件に合わせて選ぶ、という性質のもの.</p>

<ul>
<li>Language paradigm

<ul>
<li>ストリーム処理を書くための言語を何にするか. SQLは簡単だが表現力に劣る、一方で手続き型でがっつりコードを書けるようにすると自由度は高いが開発コストがかかる.</li>
<li>Facebookでは、要件に合わせてストリーム処理エンジンを選べるようになっている.</li>
</ul>
</li>
<li>Data transfer

<ul>
<li>ストリーム処理は大抵複数のオペレータから構成されるが、その間のデータ転送をどうするか.</li>
<li>直接RPCで繋ぐのは性能的に有利だが、信頼性や柔軟性には課題がある. 間にストレージを挟むのはその反対で、信頼性は高く、また送信側・受信側に処理性能の違いがあるケースに間のストレージで吸収できるというメリットがある.</li>
<li>Facebookでは間にscribeを挟む形にしている</li>
</ul>
</li>
<li>Processing semantics

<ul>
<li>障害が発生した際のログ欠損、重複に対してどういうポリシーで行くか. at-least-once, at-most-once, exactly-onceのどれかとなる.</li>
<li>statefulな処理の場合は、stateに対するsemanticsと、outputに対するsemanticsがある. statelessの場合は後者しかない</li>
<li>stateで考えると、どのsemanticsにするかは、状態の保存と読み込んでいるログのオフセット保存をどの順番で行うか、もしくはatomicに行うか、で決まる. 例えば、状態保存→オフセット保存、の場合はログの重複が発生しうるのでat-least-onceになる</li>
</ul>
</li>
<li>State saving mechanisms

<ul>
<li>処理の内部状態をどう保持するか. 耐障害性を考慮すると、障害時に復旧できる形で状態を保持する必要がある. 内部状態を常に他ノードにレプリケートしておくとか、外部のDBに保存するとか、いくつか方法が考えられる</li>
<li>Facebookで良く使われているのは、各サーバ内のDBに保存しつつ、定期的に外部データストアにもバックアップを取る、という方式. local DBに持っておけばプロセス障害などには対応できるし、最悪サーバが返ってこないケースには外部データストアのバックアップを使うことができる.</li>
<li>複数の処理ノード間での状態の整合性、というのに対しては言及がないが、Data transferのところであった通り、Facebookでは多段になっている処理の場合、間にはscribeが入っていて、各処理はscribeのオフセットを管理しているので不整合は発生しない、ということなんだろうか.</li>
</ul>
</li>
<li>Backfill Processing

<ul>
<li>過去データに対して処理を行いたい時にどうするか.</li>
<li>ストリーム処理だけでやろうとすると、過去データを全てストリームとして流せないといけないことになる. かと行って、同じ処理をバッチ用とストリーム用で二重開発するのはコストが高い.</li>
<li>Facebookは、PumaのプログラムをHive上のUDFとして再利用できるようになっている</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
</feed>
