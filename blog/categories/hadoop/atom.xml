<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Tech Notes]]></title>
  <link href="http://ogibayashi.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://ogibayashi.github.io/"/>
  <updated>2015-03-30T22:26:35+09:00</updated>
  <id>http://ogibayashi.github.io/</id>
  <author>
    <name><![CDATA[OGIBAYASHI Hironori]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDP2.2でResourceManagerが両系standbyになった事象]]></title>
    <link href="http://ogibayashi.github.io/blog/2015/01/27/hdp2-dot-2-resoruce-manager-could-not-transition-to-active/"/>
    <updated>2015-01-27T17:45:06+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2015/01/27/hdp2-dot-2-resoruce-manager-could-not-transition-to-active</id>
    <content type="html"><![CDATA[<p>HDP2.2で、daemonやクラスタの再起動を繰り返していた所、ResourceManagerが両系standbyになってしまった.</p>

<p>ResourceManagerのログには以下のように出力されている.</p>

<p>```
2015-01-16 16:29:19,495 WARN  resourcemanager.RMAuditLogger
(RMAuditLogger.java:logFailure(285)) &ndash; USER=yarn
OPERATION=transitionToActive    TARGET=RMHAProtocolService
RESULT=FAILURE  DESCRIPTION=Exception transitioning to active
PERMISSIONS=All users are allowed
2015-01-16 16:29:19,495 WARN  ha.ActiveStandbyElector
(ActiveStandbyElector.java:becomeActive(809)) &ndash; Exception handling the
winning of election
org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active</p>

<pre><code>    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)
    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:805)
    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:416)
    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
</code></pre>

<p>Caused by: org.apache.hadoop.ha.ServiceFailedException: Error when
transitioning to Active mode</p>

<pre><code>    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:304)
    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)
    ... 4 more
</code></pre>

<p>Caused by: org.apache.hadoop.service.ServiceStateException:
org.apache.hadoop.yarn.exceptions.YarnException: Application with id
application_1421115867116_0001 is already present! Cannot add a
duplicate!</p>

<pre><code>    at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:204)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)
    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)
    ... 5 more
</code></pre>

<p>Caused by: org.apache.hadoop.yarn.exceptions.YarnException:
Application with id application_1421115867116_0001 is already present!
Cannot add a duplicate!</p>

<pre><code>    at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)
    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:338)
    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:309)
    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)
    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
</code></pre>

<p>```</p>

<p>Zookeeperに変なエントリが残ってる？と思われたのでRMを停止した上で<code>/rmstore/ZKRMStateRoot/RMAppRoot/</code>以下のApplicatoinIDのエントリを全て削除.</p>

<p>```
[zk: localhost:2181(CONNECTED) 2] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421387925857_0002
[zk: localhost:2181(CONNECTED) 3] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0002
[zk: localhost:2181(CONNECTED) 4] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0003
[zk: localhost:2181(CONNECTED) 5] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0004
[zk: localhost:2181(CONNECTED) 6] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421320519530_0002
[zk: localhost:2181(CONNECTED) 7] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0005
[zk: localhost:2181(CONNECTED) 8] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421320519530_0001
[zk: localhost:2181(CONNECTED) 11] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421387925857_0001</p>

<p>```</p>

<p>で、RMを起動したら復旧した.</p>

<p>どうやら<a href="https://issues.apache.org/jira/browse/YARN-2865">YARN-2865</a>の事象っぽい. Hadoop 2.7.0でFIX. ZK上のエントリを消してしまったので、その分のジョブについてはクライアントから再投入する必要がある.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDP2.2をセットアップするためにハマった箇所のメモ]]></title>
    <link href="http://ogibayashi.github.io/blog/2015/01/06/hdp2-dot-2-installation-memo/"/>
    <updated>2015-01-06T20:53:54+09:00</updated>
    <id>http://ogibayashi.github.io/blog/2015/01/06/hdp2-dot-2-installation-memo</id>
    <content type="html"><![CDATA[<p>HDP2.2を手元のVMで試しにセットアップしてみたが、色々ハマった部分があったのでメモ</p>

<h2>環境</h2>

<p>CentOS6.3のVMを7つ用意して、以下のようにHA含めて構成することにした.</p>

<ul>
<li>master1: NameNode(active), ZKFC, JournalNode, Zookeeper</li>
<li>master2: NameNode(standby), ZKFC, JournalNode, ResourceManager(standby), Zookeeper</li>
<li>master3: JournalNode, ResourceManager(active), Zookeeper, HiveServer2, MySQL</li>
<li>slaves(3ノード): DataNode, NodeManager</li>
<li>client: MR/Tez client</li>
</ul>


<p>ドキュメントは、こちらの"Installing HDP Manually"を使用.
<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html</a></p>

<h2>トラブルシュートなどのメモ</h2>

<p>以下、ドキュメントには無いが変更しないといけなかったもの、引っかかったトラブルなど. 単に自分が手順を見落としていたり、間違っていたために発生したものもあるかも.</p>

<h3>全般</h3>

<ul>
<li>基本的に、設定はcompanion filesのものをベースにする. 2.1を動かしていた際の設定もあったが、大分変わっているようなので一旦companion filesのをまるごと持ってきた</li>
<li>インストールのベースが<code>/usr/hdp/2.2.0.0-2041/</code>になっているが、実際のスクリプトの中では<code>/usr/lib/hadoop</code>等を参照しているものもあるため、<code>/usr/hdp/2.2.0.0-2041/hadoop -&gt; /usr/hdp/2.2.0.0-2041/hadoop</code> 等のようなシンボリックリンクをひと通り作成した.</li>
<li>Daemonの起動スクリプト類は<code>hadoop-hdfs-namenode</code>等のような別RPMになっている.これらのインストール先は<code>/usr/hdp/2.2.0.0-2041/etc/</code>となっているため、<code>/etc/init.d</code>の下などに、こちらもシンボリックリンクを作成した. ちなみに、<code>/etc/default</code>の下に置くファイルも用意されているが、initスクリプトをみてもこれらを読むようには見えない.</li>
<li>マニュアルにはcompanion filesに含まれる、 <code>usersAndGroups.sh</code>, <code>directories.sh</code> を設定した上で <code>~/.bash_profile</code>でこれらのファイルを読む設定を入れるようにあるが、daemonの動作がbash_profileに依存するのが気持ち悪かったので、それはやってない. それに起因したトラブルもあるかも.</li>
</ul>


<h3>Zookeeperのセットアップ</h3>

<ul>
<li>initスクリプト内で呼ばれる、<code>zookeeper-server</code>,<code>zookeeper-server-initialize</code>は<code>/usr/hdp/2.2.0.0-2041/zookeeper/bin/</code>にあるので、これらを<code>/usr/bin</code>下に置くよう、シンボリックリンクを作成した</li>
</ul>


<h3>service <hadoop daemon> start の戻り値が3</h3>

<p>また、停止に失敗したりする.</p>

<p>以下の様な感じ.</p>

<p>```</p>

<h1>service hadoop-yarn-resourcemanager start</h1>

<p>Starting Hadoop resourcemanager:                           [  OK  ]
starting resourcemanager, logging to /var/log/hadoop/yarn/yarn-yarn-resourcemanager-hdp15.hadoop.local.out
[root@hdp15 init.d]# echo $?
3
```</p>

<p>initスクリプト内で以下のようにPIDFILEを設定しているが、環境変数が正しく設定されていないと、PIDFILEがうまく作られずにこのような状態になる.</p>

<p><code>
PIDFILE="$HADOOP_PID_DIR/yarn-$YARN_IDENT_STRING-resourcemanager.pid"
</code></p>

<p><code>yarn-env.sh</code>で<code>HADOOP_PID_DIR</code>, <code>YARN_IDENT_STRING</code>, <code>hadoop-env.sh</code>でも<code>HADOOP_PID_DIR</code>を設定するようにした.</p>

<h3>HistoryServerでPermission Deninedが発生</h3>

<p>以下の様なエラーが発生. (何をしようとして発生したのか忘れた&hellip;)</p>

<p>```
2014-12-27 03:15:05,824 ERROR hs.HistoryFileManager (HistoryFileManager.java:scanIfNeeded(285)) &ndash; Error while trying to scan the directory hdfs://hdpexperiment:
8020/mr-history/tmp/client
org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=READ_EXECUTE, inode=&ldquo;/mr-history/tmp/client&rdquo;:client:hdfs:drwxrwx&mdash;&ndash;</p>

<pre><code>    at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)
    at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)
    at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:185)
</code></pre>

<p>```</p>

<p>HDFSのパーミッションを見ると以下のようになっており、<code>/mr-history/tmp/client</code>に対して<code>mapred</code>ユーザのパーミッションが無い.</p>

<p>```</p>

<h1>sudo -u hdfs hdfs dfs -ls /</h1>

<p>Found 6 items
drwxrwxrwt   &ndash; yarn   yarn            0 2014-12-27 03:11 /app-logs
drwxr-xr-x   &ndash; hdfs   hdfs            0 2014-12-27 02:43 /apps
drwxr-xr-x   &ndash; hdfs   hadoop          0 2014-12-26 16:17 /hdp
drwxr-xr-x   &ndash; mapred hdfs            0 2014-12-26 16:16 /mr-history
drwxrwxrwt   &ndash; hdfs   hdfs            0 2014-12-27 03:11 /tmp
drwxr-xr-x   &ndash; hdfs   hdfs            0 2014-12-27 02:33 /user</p>

<h1>sudo -u hdfs hdfs dfs -ls /mr-history</h1>

<p>Found 2 items
drwxrwxrwt   &ndash; mapred hdfs          0 2014-12-26 16:16 /mr-history/done
drwxrwxrwt   &ndash; mapred hdfs          0 2014-12-26 23:27 /mr-history/tmp</p>

<h1>sudo -u hdfs hdfs dfs -ls /mr-history/tmp</h1>

<p>Found 1 items
drwxrwx&mdash;&ndash;   &ndash; client hdfs          0 2014-12-27 00:14 /mr-history/tmp/client</p>

<p>```</p>

<p>以下のように、<code>/mr-history</code>以下のgroupを<code>mapredと</code>することで対応.</p>

<p>```</p>

<h1>sudo -u hdfs hdfs dfs -chgrp -R mapred /mr-history</h1>

<p>```</p>

<h3>MapReduceジョブ実行中のエラー. Slaveにつながらない</h3>

<p>exampleのpiを実行した際のエラー</p>

<p>```
14/12/30 11:17:59 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
14/12/30 11:18:00 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
14/12/30 11:18:01 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
14/12/30 11:18:29 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
14/12/30 11:18:30 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
14/12/30 11:18:31 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
14/12/30 11:18:31 INFO mapreduce.Job: Job job_1419895534181_0002 failed with state FAILED due to: Application application_1419895534181_0002 failed 2 times due to AM Container for appattempt_1419895534181_0002_000002 exited with  exitCode: 255
For more detailed output, check application tracking page:<a href="http://hdp16.hadoop.local:8088/proxy/application_1419895534181_0002/Then,">http://hdp16.hadoop.local:8088/proxy/application_1419895534181_0002/Then,</a> click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1419895534181_0002_02_000001
Exit code: 255
Stack trace: ExitCodeException exitCode=255:</p>

<pre><code>    at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
    at org.apache.hadoop.util.Shell.run(Shell.java:455)
    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>Container exited with a non-zero exit code 255
Failing this attempt. Failing the application.
14/12/30 11:18:31 INFO mapreduce.Job: Counters: 0
Job Finished in 99.802 seconds
java.io.FileNotFoundException: File does not exist: hdfs://hdpexperiment/user/hdfs/QuasiMonteCarlo_1419905807487_296085847/out/reduce-out</p>

<pre><code>    at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1122)
    at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
    at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1750)
    at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1774)
    at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:314)
    at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:354)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:363)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
    at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
    at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
</code></pre>

<p>```</p>

<p><code>yarn logs</code>ではログが見えなかったので、実行中のサーバでコンテナのログを見てみた. <code>${hdp.version}</code>というのがそのままになっている.</p>

<p>```
2014-12-30 13:08:49,568 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.IllegalArgumentException: Unable to parse &lsquo;/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework&rsquo; as a URI, check the setting for mapreduce.application.framework.path</p>

<pre><code>    at org.apache.hadoop.mapreduce.v2.util.MRApps.getMRFrameworkName(MRApps.java:178)
    at org.apache.hadoop.mapreduce.v2.util.MRApps.setMRFrameworkClasspath(MRApps.java:203)
    at org.apache.hadoop.mapreduce.v2.util.MRApps.setClasspath(MRApps.java:248)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInitialClasspath(TaskAttemptImpl.java:620)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:755)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext(TaskAttemptImpl.java:812)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1527)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1504)
    at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1069)
    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1311)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1303)
    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>Caused by: java.net.URISyntaxException: Illegal character in path at index 11: /hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework</p>

<pre><code>    at java.net.URI$Parser.fail(URI.java:2829)
    at java.net.URI$Parser.checkChars(URI.java:3002)
    at java.net.URI$Parser.parseHierarchical(URI.java:3086)
    at java.net.URI$Parser.parse(URI.java:3044)
    at java.net.URI.&lt;init&gt;(URI.java:595)
    at org.apache.hadoop.mapreduce.v2.util.MRApps.getMRFrameworkName(MRApps.java:176)
    ... 18 more
</code></pre>

<p>```</p>

<p><code>mapred-site.xml</code>で、<code>${hdp.version}</code>となっている部分を実際のバージョン番号(<code>2.2.0.0-2041</code>)に変えたら解消した.</p>

<h3>Journalnodeの停止に失敗</h3>

<p>journalnodeを停止すると、<code>no journalnode to stop</code>というエラーが発生. ただ、プロセスは停止している.</p>

<p>```</p>

<h1>service hadoop-hdfs-journalnode stop</h1>

<p>Stopping Hadoop journalnode:                               [  OK  ]
no journalnode to stop
rm: cannot remove `/var/run/hadoop/hadoop-hdfs-journalnode.pid': Permission denied</p>

<h1>ls -l /var/run/hadoop/hadoop-hdfs-journalnode.pid</h1>

<p>-rw-r&mdash;r&mdash; 1 hdfs hdfs 6 12月 30 08:24 2014 /var/run/hadoop/hadoop-hdfs-journalnode.pid</p>

<h1>ps -ef | grep -i journal</h1>

<p>root       374     2  0 Dec29 ?        00:00:02 [kjournald]
root       811     2  0 Dec29 ?        00:00:00 [kjournald]
root     14406 14342  0 14:15 pts/0    00:00:00 grep -i journal
```</p>

<p>ディレクトリのowner/groupが<code>mapred</code>になっている.</p>

<p>```</p>

<h1>ls -ld /var/run/hadoop</h1>

<p>drwxr-xr-x 5 mapred mapred 4096 12月 30 14:16 2014 /var/run/hadoop
```</p>

<p>これを<code>hdfs.hdfs</code>にしたら事象は解消したが、HistoryServerをrestartしたら<code>/var/run/hadoop</code>のownerが<code>mapred.mapred</code>に戻ってしまった.</p>

<p><code>hadoop-mapreduce-historyserver</code>のinitスクリプトにある、以下の部分のせい. つまり、historyserverとHDFS系のdaemonが同居している場合に発生する問題.</p>

<p>```
install -d -m 0755 -o mapred -g mapred $HADOOP_PID_DIR 1>/dev/null 2>&amp;1 || :
[ -d &ldquo;$LOCKDIR&rdquo; ] || install -d -m 0755 $LOCKDIR 1>/dev/null 2>&amp;1 || :</p>

<p>```</p>

<p><code>hadoop-env.sh</code>に以下の記述を入れ、HistoryServerの<code>HADOOP_PID_DIR</code>をHDFS系と分けることで対応することにした.</p>

<p>```</p>

<h1>For HistoryServer</h1>

<p>if [ &ldquo;${SVC_USER}&rdquo; = &ldquo;mapred&rdquo; ]; then
  HADOOP_PID_DIR=/var/run/hadoop-mapreduce
fi</p>

<p>```</p>

<h3>Hive CREATE TABLE時のNo privilege</h3>

<p>beelineからhiveユーザで接続し、create tableを発行した際のエラー</p>

<p><code>
0: jdbc:hive2://hdp16.hadoop.local:10000&gt; create table test2(a int, b string);
Error: Error while compiling statement: FAILED: SemanticException MetaException(message:No privilege 'Select' found for inputs { database:default}) (state=42000,code=40000)
</code></p>

<p><a href="https://cwiki.apache.org/confluence/display/Hive/Storage+Based+Authorization+in+the+Metastore+Server">Storage Based Authorization in the Metastore Server</a>に引っかかっている模様</p>

<p>一旦以下の設定を外して対応</p>

<p>```
  <property></p>

<pre><code>&lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener&lt;/value&gt;
&lt;description&gt;List of comma separated listeners for metastore events.&lt;/description&gt;
</code></pre>

<p>  </property>
```</p>

<h3>Hive on MRでのクエリ実行エラー</h3>

<p>HiveServer2のログには以下のメッセージが出ている.</p>

<p>```
2015-01-06 03:14:42,300 ERROR [HiveServer2-Background-Pool: Thread-65]: exec.Task (SessionState.java:printError(833)) &ndash; Ended Job = job_1420474977406_0003 with
exception &lsquo;java.lang.NumberFormatException(For input string: &ldquo;100000L&rdquo;)&rsquo;
java.lang.NumberFormatException: For input string: &ldquo;100000L&rdquo;</p>

<pre><code>    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:441)
    at java.lang.Long.parseLong(Long.java:483)
    at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1189)
    at org.apache.hadoop.hive.conf.HiveConf.getLongVar(HiveConf.java:2253)
    at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.checkFatalErrors(HadoopJobExecHelper.java:209)
    at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:308)
    at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:547)
    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:435)
    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1604)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1364)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1177)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:999)
    at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:144)
    at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:69)
    at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:196)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
    at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)
    at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:208)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>```</p>

<p><a href="https://issues.apache.org/jira/browse/AMBARI-8219">AMBARI-8219</a>
の事例に従い、hive-site.xmlの以下を変更したらOK</p>

<p>```
   <property></p>

<pre><code> &lt;name&gt;hive.exec.max.created.files&lt;/name&gt;
</code></pre>

<ul>
<li> <value>100000L</value></li>
<li> <value>100000</value>
 <description>Maximum number of HDFS files created by all mappers/reducers in a MapReduce job.</description>
</property>
```</li>
</ul>


<h3>hive.execution.engine=tez でのHiveクエリ実行エラー1</h3>

<p>HiveServer2のログは以下の通り</p>

<p>```
5841 end=1419741946024 duration=183 from=org.apache.hadoop.hive.ql.Driver>
2014-12-28 13:45:46,025 ERROR [HiveServer2-Handler-Pool: Thread-56]: thrift.ProcessFunction (ProcessFunction.java:process(41)) &ndash; Internal error processing ExecuteStatement
java.lang.NoClassDefFoundError: org/apache/tez/dag/api/SessionNotRunning</p>

<pre><code>    at java.lang.Class.getDeclaredConstructors0(Native Method)
    at java.lang.Class.privateGetDeclaredConstructors(Class.java:2585)
    at java.lang.Class.getConstructor0(Class.java:2885)
    at java.lang.Class.newInstance(Class.java:350)
    at org.apache.hadoop.hive.ql.exec.TaskFactory.get(TaskFactory.java:133)
</code></pre>

<p>```</p>

<p>このクラスはtez-apiのjarに含まれているが、HiveServer2のサーバにTezクライアントをセットアップしていなかったのが原因だった. セットアップして解消.</p>

<h3>Tez OrderedWordCountの実行エラー</h3>

<p>Tezの動作確認として、tez-examplesに含まれる、OrderedWordCountを実行した際のエラー.</p>

<p>```</p>

<h1>su &ndash; hdfs</h1>

<p>-bash-4.1$ cat /tmp/test.txt
foo bar foo bar foo
$ hadoop fs -put /tmp/test.txt /tmp/test.txt
-bash-4.1$ hadoop jar /usr/hdp/2.2.0.0-2041/tez/tez-examples-0.5.2.2.2.0.0-2041.jar orderedwordcount /tmp/test.txt /tmp/out
Running OrderedWordCount
14/12/27 02:54:57 INFO client.TezClient: Tez Client Version: [ component=tez-api, version=0.5.2.2.2.0.0-2041, revision=db32ad437885baf17ab90885b4ddb226fbbe3559, SCM-URL=scm:git:<a href="https://git-wip-us.apache.org/repos/asf/tez.git,">https://git-wip-us.apache.org/repos/asf/tez.git,</a> buildTIme=20141119-1512 ]
14/12/27 02:54:59 INFO client.TezClient: Submitting DAG application with id: application_1419606523103_0010
14/12/27 02:54:59 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
14/12/27 02:54:59 INFO client.TezClientUtils: Using tez.lib.uris value from configuration: hdfs://hdpexperiment/apps/tez/,hdfs://hdpexperiment/apps/tez/lib/,hdfs://hdpexperiment/hdp/apps/current/tez/tez.tar.gz
14/12/27 02:54:59 WARN client.TezClientUtils: Duplicate resource found, resourceName=tez.tar.gz, existingPath=scheme: &ldquo;hdfs&rdquo; host: &ldquo;hdpexperiment&rdquo; port: -1 file: &ldquo;/apps/tez/lib/tez.tar.gz&rdquo;, newPath=hdfs://hdpexperiment/hdp/apps/current/tez/tez.tar.gz
14/12/27 02:54:59 INFO client.TezClient: Tez system stage directory hdfs://hdpexperiment/tmp/hdfs/staging/.tez/application_1419606523103_0010 doesn&rsquo;t exist and is created
14/12/27 02:55:00 INFO client.TezClient: Submitting DAG to YARN, applicationId=application_1419606523103_0010, dagName=OrderedWordCount
14/12/27 02:55:01 INFO impl.YarnClientImpl: Submitted application application_1419606523103_0010
14/12/27 02:55:01 INFO client.TezClient: The url to track the Tez AM: <a href="http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/">http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/</a>
14/12/27 02:55:01 INFO client.DAGClientImpl: Waiting for DAG to start running
14/12/27 02:55:13 INFO client.DAGClientImpl: DAG completed. FinalState=FAILED
OrderedWordCount failed with diagnostics: [Application application_1419606523103_0010 failed 2 times due to AM Container for appattempt_1419606523103_0010_000002 exited with  exitCode: 1
For more detailed output, check application tracking page:<a href="http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/Then,">http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/Then,</a> click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1419606523103_0010_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1:</p>

<pre><code>    at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
    at org.apache.hadoop.util.Shell.run(Shell.java:455)
    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>Container exited with a non-zero exit code 1
Failing this attempt. Failing the application.]</p>

<p>```</p>

<p>コンテナのログを見ると以下の通り. クラスが見つからない.</p>

<p>```</p>

<h1>Container: container_1419606523103_0009_01_000001 on hdp17.hadoop.local_45454</h1>

<p>LogType:stderr
Log Upload Time:27-12-2014 03:37:52
LogLength:1445
Log Contents:
Exception in thread &ldquo;main&rdquo; java.lang.NoClassDefFoundError: org/apache/hadoop/service/AbstractService</p>

<pre><code>    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)
</code></pre>

<p>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.service.AbstractService</p>

<pre><code>    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 13 more
</code></pre>

<p>```</p>

<p>このクラスは、hadoop-common.jarに入っており、<code>yarn classpath</code>で確認するとこのjarもCLASSPATHに含まれているのだが、、、</p>

<p>(1/9追記)どうやら、<code>tez.lib.uris</code>の問題だった模様.以下のように、HDFSに乗せた<code>tez.tar.gz</code>のパスを指定したらうまく動作した.</p>

<p>```</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;tez.lib.uris&lt;/name&gt;
  &lt;value&gt;/hdp/apps/current/tez/tez.tar.gz&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>```</p>

<h2>まとめ</h2>

<p>ということで、HDP2.2のクラスタを作ろうとして色々うまくいかなかったのでまとめてみた. <del>あとはTezのエラーを解消したいなぁ.</del> あと、マニュアルをブラウザで見るととても見づらいので、PDFをダウンロードして手元で見た方が良い.</p>
]]></content>
  </entry>
  
</feed>
