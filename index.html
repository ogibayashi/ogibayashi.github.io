
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Tech Notes</title>
  <meta name="author" content="OGIBAYASHI Hironori">

  
  <meta name="description" content="軽くApache Flinkの性能を測ってみた. 構成としては、Fluentd(in_tail→out_kafka_buffered)→Kafka→Flink→Elasticsearchで、仮想アクセスログ的なものに対して、URIごとの件数を1分ごとに集計して出力する、というもの。 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ogibayashi.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Tech Notes" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Tech Notes</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:ogibayashi.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/05/apache-flink-performance/">Apache Flinkの性能 - デフォルトのJSONパーサが遅かった話</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-05T10:50:12+09:00" pubdate data-updated="true">Apr 5<span>th</span>, 2016</time>
        
           | <a href="/blog/2016/04/05/apache-flink-performance/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2016/04/05/apache-flink-performance/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>軽くApache Flinkの性能を測ってみた. 構成としては、Fluentd(in_tail→out_kafka_buffered)→Kafka→Flink→Elasticsearchで、仮想アクセスログ的なものに対して、URIごとの件数を1分ごとに集計して出力する、というもの。
メッセージフォーマットはJSON。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
</span><span class='line'><span class="n">env</span><span class="o">.</span><span class="n">enableCheckpointing</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>
</span><span class='line'><span class="c1">// ....</span>
</span><span class='line'><span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="n">env</span>
</span><span class='line'>  <span class="o">.</span><span class="n">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer09</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&quot;kafka.dummy&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">json</span> <span class="k">=&gt;</span> <span class="nc">JSON</span><span class="o">.</span><span class="n">parseFull</span><span class="o">(</span><span class="n">json</span><span class="o">).</span><span class="n">get</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">AnyRef</span><span class="o">]]</span> <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&quot;uri&quot;</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">y</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>    <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="s">&quot;&quot;</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">TimeUnit</span><span class="o">.</span><span class="nc">MINUTES</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="n">currentTimeMillis</span><span class="o">(),</span> <span class="n">x</span><span class="o">)}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nc">ElasticsearchSink</span><span class="o">(</span><span class="n">config</span><span class="o">,</span> <span class="n">transports</span><span class="o">,</span> <span class="k">new</span> <span class="nc">IndexRequestBuilder</span><span class="o">[</span><span class="kt">Tuple2</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Tuple2</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]]]</span>  <span class="o">{</span>
</span><span class='line'>    <span class="k">override</span> <span class="k">def</span> <span class="n">createIndexRequest</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">Tuple2</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Tuple2</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]],</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">RuntimeContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">IndexRequest</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">json</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">AnyRef</span><span class="o">]</span>
</span><span class='line'>      <span class="n">json</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;@timestamp&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Timestamp</span><span class="o">(</span><span class="n">element</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span>
</span><span class='line'>      <span class="n">json</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;uri&quot;</span><span class="o">,</span> <span class="n">element</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
</span><span class='line'>      <span class="n">json</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="n">element</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">_2</span><span class="k">:</span> <span class="kt">java.lang.Integer</span><span class="o">)</span>
</span><span class='line'>      <span class="n">println</span><span class="o">(</span><span class="s">&quot;SENDING: &quot;</span> <span class="o">+</span> <span class="n">element</span><span class="o">)</span>
</span><span class='line'>      <span class="nc">Requests</span><span class="o">.</span><span class="n">indexRequest</span><span class="o">.</span><span class="n">index</span><span class="o">(</span><span class="s">&quot;dummy2&quot;</span><span class="o">).</span><span class="n">`type`</span><span class="o">(</span><span class="s">&quot;my-type&quot;</span><span class="o">).</span><span class="n">source</span><span class="o">(</span><span class="n">json</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}))</span>
</span></code></pre></td></tr></table></div></figure>


<p>環境は以下の通り</p>

<ul>
<li>Kafka : 8vCPU, 8GBMemoryのVM*3, HDP2.4 (Kafka-0.9)

<ul>
<li>パーティション数は1なので、実質broker1台</li>
</ul>
</li>
<li>Flink : JobManager, TaskManagerともに8vCPU, 8GBMemoryのVM (Flink-1.0.0)

<ul>
<li>TaskManagerは3台だが、ジョブ並列度を1にしたので1台でしか動かない状態</li>
</ul>
</li>
<li>Elasticsearch: 4CPU, 4GB MemoryのVM*1 (Elasticsearch 1.7.2)</li>
</ul>


<p>URIは9種類なので、Elasticsearchには毎分9レコードが出力されることにいなる。 Elasticsearchに出力されたURIごとの件数を1分単位に合計したものを、Flinkのスループットと考える。 レコードの生成には <a href="https://github.com/sonots/dummer">dummer</a>を使用した。</p>

<p>で、普通にやったら2,000msg/secの入力を与えて、89,000msg/min = 1,483msg/secしか処理できなかった。FlinkプロセスのCPUが100%(1CPU使いきり)となり、Kafkaのlagが増えていく状態。
チェックポイント外したりESへの出力をやめたりしてみたのだけど、性能はさほど変わらず。</p>

<p>何にCPUを食っているんだろう？と思って、Flinkプロセスのjstackを何回か取ってみたら、こんな感じでJSONのパースを実行中であるケースが殆んどだった。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="n">at</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">parsing</span><span class="o">.</span><span class="n">combinator</span><span class="o">.</span><span class="nc">Parsers$$anon$2</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="nc">Parsers</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">881</span><span class="o">)</span>
</span><span class='line'>    <span class="n">at</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">parsing</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="nc">JSON</span><span class="n">$</span><span class="o">.</span><span class="n">parseRaw</span><span class="o">(</span><span class="nc">JSON</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">51</span><span class="o">)</span>
</span><span class='line'>    <span class="n">at</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">parsing</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="nc">JSON</span><span class="n">$</span><span class="o">.</span><span class="n">parseFull</span><span class="o">(</span><span class="nc">JSON</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">65</span><span class="o">)</span>
</span><span class='line'>    <span class="n">at</span> <span class="nc">KafkaTest3$$anonfun$1</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="nc">KafkaTest3</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">46</span><span class="o">)</span>
</span><span class='line'>    <span class="n">at</span> <span class="nc">KafkaTest3$$anonfun$1</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="nc">KafkaTest3</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">46</span><span class="o">)</span>
</span><span class='line'>    <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">flink</span><span class="o">.</span><span class="n">streaming</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">scala</span><span class="o">.</span><span class="nc">DataStream$$anon$4</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="nc">DataStream</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">485</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>ということで、パーサをJacksonにしてみた。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">val</span> <span class="n">mapper</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ObjectMapper</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
</span><span class='line'>    <span class="n">env</span><span class="o">.</span><span class="n">enableCheckpointing</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// ...</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="n">env</span>
</span><span class='line'>     <span class="o">.</span><span class="n">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer09</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&quot;kafka.json&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">parseJson</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">x</span><span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&quot;uri&quot;</span><span class="o">).</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span> <span class="mi">1</span><span class="o">)}</span>
</span><span class='line'>      <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">TimeUnit</span><span class="o">.</span><span class="nc">MINUTES</span><span class="o">))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="n">currentTimeMillis</span><span class="o">(),</span> <span class="n">x</span><span class="o">)}</span>
</span><span class='line'>      <span class="o">.</span><span class="n">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nc">ElasticsearchSink</span><span class="o">(</span><span class="n">config</span><span class="o">,</span> <span class="n">transports</span><span class="o">,</span> <span class="k">new</span> <span class="nc">IndexRequestBuilder</span><span class="o">[</span><span class="kt">Tuple2</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Tuple2</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]]]</span>  <span class="o">{</span>
</span><span class='line'>        <span class="k">override</span> <span class="k">def</span> <span class="n">createIndexRequest</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">Tuple2</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Tuple2</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]],</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">RuntimeContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">IndexRequest</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>          <span class="k">val</span> <span class="n">json</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">AnyRef</span><span class="o">]</span>
</span><span class='line'>          <span class="n">json</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;@timestamp&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Timestamp</span><span class="o">(</span><span class="n">element</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span>
</span><span class='line'>          <span class="n">json</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;uri&quot;</span><span class="o">,</span> <span class="n">element</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
</span><span class='line'>          <span class="n">json</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="n">element</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">_2</span><span class="k">:</span> <span class="kt">java.lang.Integer</span><span class="o">)</span>
</span><span class='line'>          <span class="nc">Requests</span><span class="o">.</span><span class="n">indexRequest</span><span class="o">.</span><span class="n">index</span><span class="o">(</span><span class="s">&quot;dummy2&quot;</span><span class="o">).</span><span class="n">`type`</span><span class="o">(</span><span class="s">&quot;my-type&quot;</span><span class="o">).</span><span class="n">source</span><span class="o">(</span><span class="n">json</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>      <span class="o">}))</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&quot;KafkaTest9&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">parseJson</span><span class="o">(</span><span class="n">x</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>,<span class="kt">AnyRef</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">mapper</span><span class="o">.</span><span class="n">readValue</span><span class="o">(</span><span class="n">x</span><span class="o">,</span><span class="n">classOf</span><span class="o">[</span><span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>,<span class="kt">AnyRef</span><span class="o">]])</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>そうしたら、スループットが大幅に向上して900,000msg/min=15,000msg/secを20%程度のCPU(1CPUの5分の1)で処理できた。</p>

<p><a href="http://www.slideshare.net/nestorhp/scala-json-features-and-performance">http://www.slideshare.net/nestorhp/scala-json-features-and-performance</a> にScalaで使えるJSONライブラリを比較したスライドがあるのだけど、
これを見るとScala標準のパーサと、その他のライブラリで速度が3桁くらい違う。えーーー、、、</p>

<p>ともあれ、十分に実用的な性能が出そうであることがわかったので良かった。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/03/25/gree-tech-talk-10/">GREE Tech Talk 10に行ってきた</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-03-25T21:12:46+09:00" pubdate data-updated="true">Mar 25<span>th</span>, 2016</time>
        
           | <a href="/blog/2016/03/25/gree-tech-talk-10/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2016/03/25/gree-tech-talk-10/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://techtalk.labs.gree.jp/10/">GREE Tech Talk #10</a> に行ってきたのでメモ.</p>

<p>今回のテーマはData Visualizationということで、どんな見せ方をしたら効果的か、とかその辺の話が聞けたら良いな、と思って参加した.
実際にはそれよりも、こんなツールでこんなことできるよ、系の話が多かった. まあ、それはそれで面白かったのだけど、実際ツールは色々あるので、それらの中から適切なものを
選ぶためにも、みんな何をどう見て、どう活かしているのか、というあたりの話がもっとあると良かったなあ、というのが全体的な感想.</p>

<p>行って一番の収穫は、GREE反田さんのGrafanaにPrometheusのデータソースを追加した話だった. 自分も仕事でGrafana+Prometheusの組み合わせに出会って、
これはすごい、素晴らしいと思って最近使っているのだけど、Prometheusデータソースを作ったのが日本の方だとは知らなかった. 自分はまだProemtheusを十分に使いこなせて
いないので、懇親会の中で色々話を聞けたのは大変良かった。幾つかメモしておくと、</p>

<ul>
<li>Prometheusのアラートは任意のURLにJSONで投げることができるので、fluentdで受けて好きなところに飛ばす、ということができる</li>
<li>service discoveryにはファイルを使うことができる. つまり、例えばcronで社内の構成管理システムを叩いて、
ファイルに吐き出すようにしておけば、Prometheusがそれを随時読み込んでポーリング対象のリストを更新してくれる</li>
<li>Prometheusのクエリは結構色々できそう. 移動平均的な感じでトレンドを表示したり、変化が一定以上のものを表示したり.</li>
<li>GrafanaのTemplatingの機能も便利そう. 特定のホストグループに対してグラフを作ったり、というのが動的にできるっぽい</li>
<li>新しいメトリックを追加するのはちょっと面倒だなぁ、と思っているのだけど、そこはやはりexporterを書く感じになるらしい.</li>
</ul>


<p>Prometheusの話以外だと、<a href="http://e2d3.org/ja/">E2D3</a>というのも面白そうだった。Excelを使って、d3.js等を使った色々な可視化をすることができる.
可視化のパターンはテンプレートという形で誰でも作って公開できるので、既存のテンプレートを使って、そこにExcelから値を埋めて、、、という感じで
簡単に、見た目のインパクトがある素敵なグラフなどを作ることができるとのこと.
Excel、好きではないけどやっぱり手軽で素晴らしいツールなので、これとE2D3を使うと結構楽しそうだな、と思った.</p>

<p>自分はインフラとかミドルウェアが中心で、あまりUIを作る人では無いのだけど、処理した結果のデータから最終的な価値を引き出すところは可視化がやっぱり鍵で、
こんな風に見たいから、こんなデータを集めないと、とか、大量のデータを素早く処理したい、とか、もっとリアルタイムに、とか裏側のモチベーションに繋がる
ところがあると思う. いい感じの裏側作っても、見た目が無いと伝わらないし.</p>

<p>あと、会場では飲み物とお菓子が豊富でビックリした。無料で興味深い話を聞けた上に、懇親会まで含めて飲み食いも無料でできるとは、なんて素晴らしいんだと
思った勉強会でした. 関係者の皆さんに感謝です.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/02/26/trying-apache-flink/">Apache Flinkを試している</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-02-26T20:38:01+09:00" pubdate data-updated="true">Feb 26<span>th</span>, 2016</time>
        
           | <a href="/blog/2016/02/26/trying-apache-flink/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2016/02/26/trying-apache-flink/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>耐障害性と拡張性のあるストリーム処理基盤が欲しい、と思って<a href="https://flink.apache.org/">Apache Flink</a>を調べている. 今はリアルタイム集計に<a href="http://norikra.github.io/">Norikra</a>を使っていて、これはとてもカジュアルに使えて良いのだけど、以下の様なケースだと難しい。</p>

<ul>
<li>比較的止めたくない処理で、サーバ障害時にも自動的に回復して欲しい</li>
<li>1日とか長いtime windowの集計をしているので、途中でサーバが落ちて集計中の状態が失われると辛い</li>
<li>トラフィックが増えてきて、複数サーバに負荷を分散したい</li>
<li>例えばストリームに含まれているIDに対応する値を外部のテーブルから取ってくるような、ちょっと複雑な処理をしたい</li>
</ul>


<h3>Flinkとはどのようなソフトウェアか</h3>

<p>一言で言うと、対障害性と拡張性を備えた、分散ストリーム処理基盤。バッチ処理もストリーム処理の仕組みでできるよね、ということでバッチ用、ストリーム用両方のAPIが提供されている。実行環境としては、Hadoop等と同じようにワーカプロセスが複数のサーバで立ち上がっていて、そこに対してジョブを投げるような感じになっている。バッチではジョブを投げるとデータを処理して終了、だけど、ストリーム処理では投げたジョブはずっと生きていて、ストリームにデータが流れてくるたびにそのデータを処理して出力する、という形になる。ジョブは、JavaとScalaで書くことができる。ロードマップにはSQL的なものもあるっぽいけど、今は存在しない。</p>

<h3>SparkとかStormと何が違うの？</h3>

<p>ググると色々出てくるが、<a href="https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at">米Yahooのベンチマーク</a>とか、<a href="http://www.altaterra.net/blogpost/288668/225612/Which-Stream-Processing-Engine-Storm-Spark-Samza-or-Flink">このページ</a>が分かりやすかった。</p>

<p>Stromとの比較で言うと、処理形態はほぼ同じ。ただ、大きな違いとしてStormでは各処理オペレータはstatelessになっていて、落ちると状態が失われる。永続化したかったら、自分で外部ストレージを使うようなコードを書く必要がある。耐障害性ということろだと、各レコードに対して処理が完了した際にackを返す仕組みがあるので、障害などでackが無かったら再度処理する、というような処理を書く感じになる。なので、例えば処理が途中まで行われて障害になると、そのレコードは重複して処理される形になる。Flinkは<a href="https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.html">ここ</a>に詳しく書いてあるけど、オペレータがstatefulになっていて、チェックポイントごとに状態が保存される。障害時には、チェックポイントから復旧し、それ以降のレコードをリプレイすることで、exactly-onceを実現している。</p>

<p>あとは、Stormだと各処理オペレータ(Bolt)をJavaのクラスとして書かないといけないけど、Flinkは<code>stream.keyBy(...).map{...}.timeWindow(...)</code>みたいな感じのハイレベルのAPIが提供されている。</p>

<p>Spark Streamingとの違いで言うと、Spark Streamingは正確にはストリーミングではなくてマイクロバッチなので、そのバッチの間隔にストリーム処理のwindowが左右される。sliding time windowとか、一定数のレコードを保持するようなwindowは作ることができない。(ちゃんとドキュメント読んでないけど、そのはず)</p>

<h3>触ってみた</h3>

<p>とりあえず触ってみるには、<a href="https://ci.apache.org/projects/flink/flink-docs-release-0.10/quickstart/setup_quickstart.html">https://ci.apache.org/projects/flink/flink-docs-release-0.10/quickstart/setup_quickstart.html</a> に従えば良い。</p>

<p>ざっくりした流れとしては、</p>

<ul>
<li>バイナリをダウンロードして展開</li>
<li><code>./bin/start-local.sh</code> 実行</li>
<li><a href="http://localhost:8081/">http://localhost:8081/</a> でJobManagerを開く</li>
<li>exampleがバイナリと一緒に配布されているので、それを実行</li>
</ul>


<p>という感じで試すことができる。</p>

<p>クラスタを組むのも割と簡単で、 <a href="https://ci.apache.org/projects/flink/flink-docs-release-0.10/setup/cluster_setup.html">https://ci.apache.org/projects/flink/flink-docs-release-0.10/setup/cluster_setup.html</a> に従えばできる。</p>

<p>JobManagerというのがマスタで、TaskManagerというのがワーカになっているので、これをサーバに分散配置することになる。</p>

<p>流れとしては、</p>

<ul>
<li>各サーバにflink用のユーザとssh keyを用意

<ul>
<li>ssh keyは、起動スクリプトの中でssh経由でTaskManagerを起動するので必要.自分で各サーバのTaskManagerを起動して回るなら無くても良い</li>
</ul>
</li>
<li>各サーバにバイナリを配布</li>
<li>設定ファイル(flink-conf.yaml, slaves)を用意

<ul>
<li>flink-conf.yamlは、配布されているものをそのまま使って、JobManagerのアドレスを設定すれば良い</li>
<li>slavesはTaskManagerホストを列挙したもの。クラスタ起動スクリプトの中で使われる</li>
</ul>
</li>
</ul>


<p>となる。
  ジョブを書くには、<a href="https://ci.apache.org/projects/flink/flink-docs-release-0.10/quickstart/scala_api_quickstart.html">https://ci.apache.org/projects/flink/flink-docs-release-0.10/quickstart/scala_api_quickstart.html</a> に従う。mavenのアーキタイプが用意されてるので、それでプロジェクトのテンプレートを作って、ドキュメントの中にあるWordCountをコピーして走らせれば良い。</p>

<h3>感想</h3>

<p>試しにfluend→Kafka→Flink→Elasticsearch、とつなげて分散実行してみたり、プロセスを落としてみたりしたが、期待通りに動作した。例えば、5分間のtime windowで件数を集計するような処理を作って、実行中にプロセスを落とすと、障害が検知されてジョブが再実行される。そして、勝手に必要なリカバリがされるので、障害があっても無くても実行結果は変わらない。すごい。(もう少しちゃんと見ると、ずれるケースもありそうだけど、まだ詳しく見ていない)</p>

<p>並列度は、ジョブの投入時に指定できるので、ちゃんとKafkaのパーティションを複数作って並列度を指定して実行すれば、各TaskManagerに分散して実行される。</p>

<p>ただ、性能のところが今一つで、処理を3サーバに分散、5,000msg/secを投入してやってみたら、各サーバ1CPUを100%使って、Kafkaにメッセージが滞留する状態になった。実際に処理できたのは4,300msg/secくらい。3CPUで4,300msg/secだと大分コストが高いなあ、という印象。まあ、とりあえず動かしてみただけなので、何か正しくない可能性はある。もう少し試してみたい。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/02/08/hadoop-slash-spark-conference-2016/">Hadoop / Spark Conference Japan 2016 に行ってきた</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-02-08T19:55:02+09:00" pubdate data-updated="true">Feb 8<span>th</span>, 2016</time>
        
           | <a href="/blog/2016/02/08/hadoop-slash-spark-conference-2016/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2016/02/08/hadoop-slash-spark-conference-2016/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>行ってきた。本当はストリーム処理をする上でのSparkってどうよ、的なあたりを聞きたかったのだけど、気がついたらHadoop周りの話題を多く聞いていたように思う。後で資料が公開されたら、行くことができなかったセッションの内容も見ておきたい。</p>

<p>以下、思ったことなどをつらつらと書いてみる。</p>

<p>ひとつ目は、これからはCPUをどうするかというのが大きい課題なのかなぁ、ということ。Sparkの、Project Tungstenの話の中でも出てきたけど、ハードウェアの進化という面だと、ディスクはSSDが普及してきたし、ネットワークは10Gbpsになってきたし、メモリは安価・大容量になったけど、CPUは劇的に速くなっていない。分散処理とか、GPUを使うとか、CPUの集積度を上げる、とかしていっても、最後には消費電力という課題が残る。数年前まではディスクI/Oがボトルネックで、これをどうするか、的な感じだったけど、その辺がある程度解決してきたのと、機械学習周りの普及で、今はCPUをいかに効率よく回すか、ということが重要になってきているのだなぁ、というのを感じた。</p>

<p>Sparkについては、懇親会の中で少し聞いたところだと、他の人が踏み固めた道を歩く分には割と大丈夫、という感じらしい。一般的なユースケースと違う使い方をしたり、リソース的にシビアだったりすると辛いのかな、と。これはSparkに限らない気はするけど。挙動が分かりやすいとか、今どうなっているのかが把握しやすいとか、再起動のしやすさとかの、運用の容易さは重要だよな、と思ったけど、そこは実際に動かしてみないとわからなそうだな。使っている人たちにもう少し詳しく聞けばよかった、というのは反省。</p>

<p>Hadoopは、まあ成熟している印象。使っていて大変だった系の話としては、Yahooさんの数百ノードまでスケールさせた時にHive MetastoreとかYARN Resource Managerに負荷が集中し話とか、Softbankさんの、オンプレでサーバの物理的な構築とか、故障対応のオペレーションの話があった。自分のところでも使っているけど、そこそこ余裕のある構成で組んで、普通に集計で使用している分には問題ないなあ、と思う。構築・運用周りも前は自分でがんばってたけど、Ambariを使い始めてから大分楽になった。もちろん、Hadoop自体が停滞しているわけではなくてYARNもHDFSもHiveも改善されて性能が上がったり、使いやすくなったりはしているわけで、それは使っている身としてはとてもありがたい。</p>

<p>あと、本題とは関係ないけど、懇親会の中で同僚と話していて、日頃仕事でやっていることとか、公開できる範囲でブログに書いて世の中に共有することは大事だよね、という話になった。自分もそういう記事に多分にお世話になっているので。ということで、まずは忘れないうちにこの記事を書いておこう、と思った次第です。(誰かの役は立たなそうだけど、まずは書く、ということで)</p>

<p>最後に、運営に関わられた方々、発表者の方々、どうもありがとうございました。各発表も、運営周りも、ランチも懇親会も素晴らしかったです。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/08/13/esper-and-java-method/">EsperのEPLでjava methodとGROUP Byを使ったらレコードが重複した話</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-08-13T10:19:29+09:00" pubdate data-updated="true">Aug 13<span>th</span>, 2015</time>
        
           | <a href="/blog/2015/08/13/esper-and-java-method/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2015/08/13/esper-and-java-method/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Norikraで以下の様な感じの、GROUP BYして、SELECTの中でjavaメソッドを使うようなクエリを登録していたのだけど、なんか生成されるイベントが多い、というのが発端で調べてみた.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SELECT str, str.split(",") as splitted, count(*) as cnt FROM str_stream.win:time_batch(10 sec) GROUP BY str</span></code></pre></td></tr></table></div></figure>


<p>上記のクエリを登録した状態で、以下のようにイベントを投入する.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ echo '{"str":"a,b,c"}' | norikra-client event send str_stream
</span><span class='line'>$ echo '{"str":"a,b,c"}' | norikra-client event send str_stream
</span><span class='line'>$ echo '{"str":"a,b,c"}' | norikra-client event send str_stream</span></code></pre></td></tr></table></div></figure>


<p>と、こんな感じで同じようなレコードが重複して生成される. <code>count(*)</code>は正しくカウントされているのだが、GROUP BYしているので1レコードになってて欲しい.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ norikra-client event sweep
</span><span class='line'>{"time":"2015/07/31 12:06:52","query":"splittest","str":"a,b,c","splitted":["a","b","c"],"cnt":3}
</span><span class='line'>{"time":"2015/07/31 12:06:52","query":"splittest","str":"a,b,c","splitted":["a","b","c"],"cnt":3}
</span><span class='line'>{"time":"2015/07/31 12:06:52","query":"splittest","str":"a,b,c","splitted":["a","b","c"],"cnt":3}</span></code></pre></td></tr></table></div></figure>


<p>Norikraではなく、Esperで同じことを実験してみても発生するので、Esperの事象っぽい.<a href="http://www.espertech.com/esper/release-5.2.0/esper-reference/html/processingmodel.html">公式ドキュメント</a>の&#8221;3.7.2. Output for Aggregation and Group-By&#8221;を見ると、以下の様な記述がある。 GROUP BYの中に無いフィールドをSELECTすると、入力イベントと同じ数の出力イベントが発生するよ、という話.</p>

<blockquote><p>If your statement selects non-aggregated properties and aggregation values, and groups only some properties using the group by clause, your statement may look as below:</p>

<p> select account, accountName, sum(amount)
from Withdrawal.win:time_batch(1 sec)
group by account</p>

<p>At the end of a time interval, the engine posts to listeners one row per event. The aggregation result aggregates per unique account.</p></blockquote>

<p>とは言え、GROUP BYの中に<code>str.split(",")</code>を入れても同じ. 値は同じでも、Stringクラスのオブジェクトは別になってしまうからなのかなぁ. 一応、回避策としてはfirsteverという、イベントの中で最初の値だけを取る関数があるので、<code>firstever(str.split(","))</code>のようにすると1レコードに集約できた.</p>

<p>Issueで聞いてみた.
<a href="https://github.com/espertechinc/esper/issues/18">https://github.com/espertechinc/esper/issues/18</a></p>

<p>若干うまく伝わらなかった気がするけど、javaメソッドではなく同等の機能を持つUDFを作ればうまくいくよ、ということらしい。実際、EsperのUDFを作ってみたら、ちゃんと集約された。</p>

<p>ちなみに、EsperのUDFを作るのは簡単で、以下のようにstaticなメソッドを持つクラスを作って</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MyUtilityClass</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="n">String</span><span class="o">[]</span> <span class="nf">split</span><span class="o">(</span><span class="n">String</span> <span class="n">str</span><span class="o">,</span> <span class="n">String</span> <span class="n">regex</span><span class="o">){</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">str</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="n">regex</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>ConfigurationクラスのaddPlugInSingleRowFunctionメソッドで、function名、クラス名、メソッド名を渡してやれば良い。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">addPlugInSingleRowFunction</span><span class="o">(</span><span class="s">&quot;split&quot;</span><span class="o">,</span> <span class="n">MyUtilityClass</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span> <span class="s">&quot;split&quot;</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>まあ、都度UDF作るのも面倒なので、firsteverで回避出来る場合はそうするかなぁ.</p>

<p>そんな話でした.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/24/code-reading-of-fluentd-v0-dot-12-filter-and-label-related/">FluentdのFilter&Label周りのコードを読んだメモ</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-03-24T13:45:46+09:00" pubdate data-updated="true">Mar 24<span>th</span>, 2015</time>
        
           | <a href="/blog/2015/03/24/code-reading-of-fluentd-v0-dot-12-filter-and-label-related/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2015/03/24/code-reading-of-fluentd-v0-dot-12-filter-and-label-related/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Fluentd v0.12の目玉機能としてFilterとLabelがある. この機能の導入にあたってはメッセージのルーティングを行う部分のコードがガラリと変わっているはずなので、興味本位で読んでみた.</p>

<h2>機能についての参考文書</h2>

<p>そもそもFilterやLabelって何？というあたりは以下が参考になる。</p>

<ul>
<li><a href="http://repeatedly.github.io/ja/2014/08/fluentd-filter-and-label/">Fluentd v0.12でのFilterとLabel</a></li>
<li><a href="http://www.fluentd.org/blog/fluentd-v0.12-is-released">Fluentd v0.12 is Released</a></li>
<li><a href="http://qiita.com/muran001/items/f73d19398b750abb9c2d">Fluentd v0.12の目玉機能らしいFilterを試してみた</a></li>
<li><a href="http://qiita.com/sonots/items/a01d2233210b7b059967">Fluentd v0.12 ラベル機能の使い方とプラグインの改修方法</a></li>
</ul>


<h2>v0.10ではどうだったか</h2>

<p>Matchクラスが<code>match</code>ディレクティブで宣言されたタグのパターンと、行き先のOutputクラスを保持していて、<code>EngineClass#emit</code> (最終的な呼び出し先は<code>emit_stream</code>)で該当するMatchを探し出し、そこに向けて<code>emit</code>する、という形だった.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'>  <span class="k">def</span> <span class="nf">emit_stream</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">)</span>
</span><span class='line'>      <span class="n">target</span> <span class="o">=</span> <span class="vi">@match_cache</span><span class="o">[</span><span class="n">tag</span><span class="o">]</span>
</span><span class='line'>      <span class="k">unless</span> <span class="n">target</span>
</span><span class='line'>        <span class="n">target</span> <span class="o">=</span> <span class="n">match</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span> <span class="o">||</span> <span class="no">NoMatchMatch</span><span class="o">.</span><span class="n">new</span>
</span><span class='line'>        <span class="c1"># this is not thread-safe but inconsistency doesn&#39;t</span>
</span><span class='line'>        <span class="c1"># cause serious problems while locking causes.</span>
</span><span class='line'>        <span class="k">if</span> <span class="vi">@match_cache_keys</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;=</span> <span class="no">MATCH_CACHE_SIZE</span>
</span><span class='line'>          <span class="vi">@match_cache</span><span class="o">.</span><span class="n">delete</span> <span class="vi">@match_cache_keys</span><span class="o">.</span><span class="n">shift</span>
</span><span class='line'>        <span class="k">end</span>
</span><span class='line'>        <span class="vi">@match_cache</span><span class="o">[</span><span class="n">tag</span><span class="o">]</span> <span class="o">=</span> <span class="n">target</span>
</span><span class='line'>        <span class="vi">@match_cache_keys</span> <span class="o">&lt;&lt;</span> <span class="n">tag</span>
</span><span class='line'>      <span class="k">end</span>
</span><span class='line'>      <span class="n">target</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>なので、ルーティングを管理するテーブルは<code>EngineClass</code>の<code>@matches</code>一つだったし、tagがルーティングのキーになっていた. 複数のOutputで順番に処理したい場合は都度tagを書き換えていく必要があった.</p>

<h2>v0.12の場合</h2>

<p><code>Agent</code>, <code>RootAgent</code>, <code>Label</code>, <code>EventRouter</code>と言った新しいクラスが導入されている.</p>

<ul>
<li><code>Label</code>

<ul>
<li>各<code>label</code>ディレクティブの中に存在するFilterおよびOutputプラグインを管理するクラス</li>
</ul>
</li>
<li><code>RootAgent</code>

<ul>
<li><code>label</code>ディレクティブに属さない(設定ファイルのルート直下にある)Input, Filter, Outputプラグイン、および各Labelクラスを管理するクラス</li>
</ul>
</li>
<li><code>Agent</code>

<ul>
<li><code>RootAgent</code>および<code>Label</code>の親クラス.</li>
</ul>
</li>
<li><code>EventRouter</code>

<ul>
<li>ルーティングのためのルール(どのタグパターンに対して、どのようなfilterやmatchが存在するか)を管理し、イベントのルーティングを行うクラス</li>
</ul>
</li>
</ul>


<p><code>RootAgent</code>および<code>Label</code>のインスタンスは、それぞれ自分自身が管理する範囲のルーティングを行う<code>EventRouter</code>クラスのインスタンスを保持している.</p>

<p>以下、実際のコードを見てみる.</p>

<h2>起動部分</h2>

<p>まずは、configurationを読み込んでいく段階でどのようなクラスが生成されていくのかを見てみる.</p>

<h3>Supervisor#start (init_engine)</h3>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/supervisor.rb#L124">Supervisor#start</a></p>

<p>Supervisorが起動して、色々と準備していく部分. <code>init_engine</code>内で <a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/engine.rb#L41">Engine#init</a> が呼ばれ、ここで<code>RootAgent</code>が生成される.</p>

<p><code>RootAgent</code>の親クラスである<code>Agent</code>のコンストラクタは以下のようになっている</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/agent.rb#L29">Agent#initialize</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">opts</span> <span class="o">=</span> <span class="p">{})</span>
</span><span class='line'>  <span class="k">super</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'>  <span class="vi">@context</span> <span class="o">=</span> <span class="kp">nil</span>
</span><span class='line'>  <span class="vi">@outputs</span> <span class="o">=</span> <span class="o">[]</span>
</span><span class='line'>  <span class="vi">@filters</span> <span class="o">=</span> <span class="o">[]</span>
</span><span class='line'>  <span class="vi">@started_outputs</span> <span class="o">=</span> <span class="o">[]</span>
</span><span class='line'>  <span class="vi">@started_filters</span> <span class="o">=</span> <span class="o">[]</span>
</span><span class='line'>
</span><span class='line'>  <span class="vi">@log</span> <span class="o">=</span> <span class="no">Engine</span><span class="o">.</span><span class="n">log</span>
</span><span class='line'>  <span class="vi">@event_router</span> <span class="o">=</span> <span class="no">EventRouter</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="no">NoMatchMatch</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">log</span><span class="p">),</span> <span class="nb">self</span><span class="p">)</span>
</span><span class='line'>  <span class="vi">@error_collector</span> <span class="o">=</span> <span class="kp">nil</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>自身が管理するOutputクラス、Filterクラス達を保持するための変数が存在している. また、そのスコープでのルーティングを行う<code>EventRouter</code>クラスをここで生成している.</p>

<p><code>RootAgent</code>については、これに加えて更にInputやLabelクラスも管理するような構造になっている. (<a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/root_agent.rb#L46">root_agent.rb</a>)</p>

<h3>Supervisor#start (run_configure)</h3>

<p><code>Supervisor#run_conigure</code>が呼ばれると、<code>Engine#configure</code>を経由して<code>RootAgent#configure</code>が呼ばれる.</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/root_agent.rb#L62">RootAgent#configure</a></p>

<p>ちょっと長いが引用.これにより以下が行われる</p>

<ul>
<li>labelディレクティブがあった場合

<ul>
<li> <a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/root_agent.rb#L155">add_label</a>により新規<code>Label</code>オブジェクトを生成</li>
<li> さらに、その<code>Label</code>オブジェクトの<code>configure</code>を呼び出す. <code>configure</code>の内容については、以下の<code>Agent#configure</code>を参照.</li>
</ul>
</li>
<li>sourceディレクティブがあった場合

<ul>
<li><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/root_agent.rb#L141">add_source</a>によりInputプラグインのインスタンスを生成</li>
<li>Inputプラグインがemitする際の投げ先として、以下を登録.

<ul>
<li>そのInputプラグインで<code>@label</code>が設定されている場合→設定された<code>Label</code>オブジェクトの<code>EventRouter</code>を登録</li>
<li>それ以外の場合→<code>RootAgent</code>の<code>EventRouter</code>を登録</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Inputプラグインについては<code>@label</code>が設定されている場合とそうでない場合で、emit先の<code>EventRouter</code>を切り替えることができるようになっている.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">configure</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
</span><span class='line'>  <span class="n">error_label_config</span> <span class="o">=</span> <span class="kp">nil</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># initialize &lt;label&gt; elements before configuring all plugins to avoid &#39;label not found&#39; in input, filter and output.</span>
</span><span class='line'>  <span class="n">label_configs</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class='line'>  <span class="n">conf</span><span class="o">.</span><span class="n">elements</span><span class="o">.</span><span class="n">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span> <span class="n">e</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;label&#39;</span> <span class="p">}</span><span class="o">.</span><span class="n">each</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span>
</span><span class='line'>    <span class="nb">name</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">arg</span>
</span><span class='line'>    <span class="k">raise</span> <span class="no">ConfigError</span><span class="p">,</span> <span class="s2">&quot;Missing symbol argument on &lt;label&gt; directive&quot;</span> <span class="k">if</span> <span class="nb">name</span><span class="o">.</span><span class="n">empty?</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="nb">name</span> <span class="o">==</span> <span class="no">ERROR_LABEL</span>
</span><span class='line'>      <span class="n">error_label_config</span> <span class="o">=</span> <span class="n">e</span>
</span><span class='line'>    <span class="k">else</span>
</span><span class='line'>      <span class="n">add_label</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
</span><span class='line'>      <span class="n">label_configs</span><span class="o">[</span><span class="nb">name</span><span class="o">]</span> <span class="o">=</span> <span class="n">e</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="c1"># Call &#39;configure&#39; here to avoid &#39;label not found&#39;</span>
</span><span class='line'>  <span class="n">label_configs</span><span class="o">.</span><span class="n">each</span> <span class="p">{</span> <span class="o">|</span><span class="nb">name</span><span class="p">,</span> <span class="n">e</span><span class="o">|</span> <span class="vi">@labels</span><span class="o">[</span><span class="nb">name</span><span class="o">].</span><span class="n">configure</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="p">}</span>
</span><span class='line'>  <span class="n">setup_error_label</span><span class="p">(</span><span class="n">error_label_config</span><span class="p">)</span> <span class="k">if</span> <span class="n">error_label_config</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">super</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># initialize &lt;source&gt; elements</span>
</span><span class='line'>  <span class="k">if</span> <span class="vi">@without_source</span>
</span><span class='line'>    <span class="n">log</span><span class="o">.</span><span class="n">info</span> <span class="s2">&quot;&#39;--without-source&#39; is applied. Ignore &lt;source&gt; sections&quot;</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="n">conf</span><span class="o">.</span><span class="n">elements</span><span class="o">.</span><span class="n">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span> <span class="n">e</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;source&#39;</span> <span class="p">}</span><span class="o">.</span><span class="n">each</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span>
</span><span class='line'>      <span class="n">type</span> <span class="o">=</span> <span class="n">e</span><span class="o">[</span><span class="s1">&#39;@type&#39;</span><span class="o">]</span> <span class="o">||</span> <span class="n">e</span><span class="o">[</span><span class="s1">&#39;type&#39;</span><span class="o">]</span>
</span><span class='line'>      <span class="k">raise</span> <span class="no">ConfigError</span><span class="p">,</span> <span class="s2">&quot;Missing &#39;type&#39; parameter on &lt;source&gt; directive&quot;</span> <span class="k">unless</span> <span class="n">type</span>
</span><span class='line'>      <span class="n">add_source</span><span class="p">(</span><span class="n">type</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>さらに、<code>RootAgent</code>の親クラスである<code>Agent#configure</code>では同様にmatchに対して<code>add_match</code>、filterについて<code>add_filter</code>が呼ばれる.</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/agent.rb#L50">Anget#configure</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">configure</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
</span><span class='line'>  <span class="k">super</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># initialize &lt;match&gt; and &lt;filter&gt; elements</span>
</span><span class='line'>  <span class="n">conf</span><span class="o">.</span><span class="n">elements</span><span class="o">.</span><span class="n">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span> <span class="n">e</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;filter&#39;</span> <span class="o">||</span> <span class="n">e</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;match&#39;</span> <span class="p">}</span><span class="o">.</span><span class="n">each</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span>
</span><span class='line'>    <span class="n">pattern</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">empty?</span> <span class="p">?</span> <span class="s1">&#39;**&#39;</span> <span class="p">:</span> <span class="n">e</span><span class="o">.</span><span class="n">arg</span>
</span><span class='line'>    <span class="n">type</span> <span class="o">=</span> <span class="n">e</span><span class="o">[</span><span class="s1">&#39;@type&#39;</span><span class="o">]</span> <span class="o">||</span> <span class="n">e</span><span class="o">[</span><span class="s1">&#39;type&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="k">if</span> <span class="n">e</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;filter&#39;</span>
</span><span class='line'>      <span class="n">add_filter</span><span class="p">(</span><span class="n">type</span><span class="p">,</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span><span class='line'>    <span class="k">else</span>
</span><span class='line'>      <span class="n">add_match</span><span class="p">(</span><span class="n">type</span><span class="p">,</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>Agent</code>は<code>Label</code>の親クラスでもあるので、新しい<code>Label</code>オブジェクトの<code>configure</code>が呼び出された際もこのコードが実行されることになる.</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/agent.rb#L134">add_filter</a>や<a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/agent.rb#L122">add_match</a>が何をしているかというと、その<code>Agent</code>が持っている<code>EventRouter</code>に対してルーティングのルール(<code>Rule</code>オブジェクト)を登録している.</p>

<h3>絵にすると、、、</h3>

<p><a href="http://www.fluentd.org/blog/fluentd-v0.12-is-released">FluentdのBlog</a>に書かれているサンプルを元に、どんな感じのオブジェクトたちが出来上がるかを絵にするとこんな感じ.</p>

<p><img src="/images/2015/201503_Fluentd_Routing.png" alt="RootAgent" /></p>

<p>labelごとにルーティングテーブルを持つので、labelが違えば異なるルールでルーティングする、ということができるようになる.</p>

<h2>emitの動き</h2>

<p>ここまでで、各Input, Filter, Outputプラグインインスタンスは、自分がemitする先の<code>EventRouter</code>オブジェクトを知っていることになる.</p>

<p>まず、Inputプラグイン内では自分が知っている<code>EventRouter</code>に<code>emit</code>する.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">router</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">record</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>emit</code>は<code>emit_stream</code>に飛ぶので、以下のコードが呼び出される. <code>match</code>メソッドが返してきたオブジェクトに対して<code>emit</code>する.</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/event_router.rb#L87">EventRouter#emit_stream</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">emit_stream</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">)</span>
</span><span class='line'>  <span class="n">match</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">,</span> <span class="vi">@chain</span><span class="p">)</span>
</span><span class='line'><span class="k">rescue</span> <span class="o">=&gt;</span> <span class="n">e</span>
</span><span class='line'>  <span class="vi">@emit_error_handler</span><span class="o">.</span><span class="n">handle_emits_error</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/event_router.rb#L101">EventRouter#match</a> に飛ぶ. <code>match</code>はemitされたtagを受け取るべきCollectorを返す.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">match</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
</span><span class='line'>  <span class="n">collector</span> <span class="o">=</span> <span class="vi">@match_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="n">c</span> <span class="o">=</span> <span class="n">find</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span> <span class="o">||</span> <span class="vi">@default_collector</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="n">collector</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>このCollectorを探す部分がどうなっているかと言うと、</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/event_router.rb#L156">event_router#find</a></p>

<p>こんな感じになっている. つまり、Filterが使われていれば<code>Pipeline</code>オブジェクトを生成してそこにFilterやOutputを順次追加していく. Filterがなければ<code>Pipeline</code>の代わりにOutputを直接返す.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">find</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
</span><span class='line'>  <span class="n">pipeline</span> <span class="o">=</span> <span class="kp">nil</span>
</span><span class='line'>  <span class="vi">@match_rules</span><span class="o">.</span><span class="n">each_with_index</span> <span class="p">{</span> <span class="o">|</span><span class="n">rule</span><span class="p">,</span> <span class="n">i</span><span class="o">|</span>
</span><span class='line'>    <span class="k">if</span> <span class="n">rule</span><span class="o">.</span><span class="n">match?</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
</span><span class='line'>      <span class="k">if</span> <span class="n">rule</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">is_a?</span><span class="p">(</span><span class="no">Filter</span><span class="p">)</span>
</span><span class='line'>        <span class="n">pipeline</span> <span class="o">||=</span> <span class="no">Pipeline</span><span class="o">.</span><span class="n">new</span>
</span><span class='line'>        <span class="n">pipeline</span><span class="o">.</span><span class="n">add_filter</span><span class="p">(</span><span class="n">rule</span><span class="o">.</span><span class="n">collector</span><span class="p">)</span>
</span><span class='line'>      <span class="k">else</span>
</span><span class='line'>        <span class="k">if</span> <span class="n">pipeline</span>
</span><span class='line'>          <span class="n">pipeline</span><span class="o">.</span><span class="n">set_output</span><span class="p">(</span><span class="n">rule</span><span class="o">.</span><span class="n">collector</span><span class="p">)</span>
</span><span class='line'>        <span class="k">else</span>
</span><span class='line'>          <span class="c1"># Use Output directly when filter is not matched</span>
</span><span class='line'>          <span class="n">pipeline</span> <span class="o">=</span> <span class="n">rule</span><span class="o">.</span><span class="n">collector</span>
</span><span class='line'>        <span class="k">end</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">pipeline</span>
</span><span class='line'>      <span class="k">end</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="n">pipeline</span>
</span><span class='line'>    <span class="c1"># filter is matched but no match</span>
</span><span class='line'>    <span class="n">pipeline</span><span class="o">.</span><span class="n">set_output</span><span class="p">(</span><span class="vi">@default_collector</span><span class="p">)</span>
</span><span class='line'>    <span class="n">pipeline</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="kp">nil</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>よって、<code>emit</code>されたレコードは<code>Pipeline</code>または<code>Output</code>に<code>emit</code>されることになる. そして、<code>Pipeline</code>に<code>emit</code>された場合は以下のコードに辿り着き、順番にFilterを通った後に最終的にOutputに<code>emit</code>されることになる.</p>

<p><a href="https://github.com/fluent/fluentd/blob/f6aa9a4b275e4e9885bb912bcf0e930966d8e246/lib/fluent/event_router.rb#L147">Pipeline#emit</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'>  <span class="k">def</span> <span class="nf">emit</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">,</span> <span class="n">chain</span><span class="p">)</span>
</span><span class='line'>    <span class="n">processed</span> <span class="o">=</span> <span class="n">es</span>
</span><span class='line'>    <span class="vi">@filters</span><span class="o">.</span><span class="n">each</span> <span class="p">{</span> <span class="o">|</span><span class="n">filter</span><span class="o">|</span>
</span><span class='line'>      <span class="n">processed</span> <span class="o">=</span> <span class="n">filter</span><span class="o">.</span><span class="n">filter_stream</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">processed</span><span class="p">)</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="vi">@output</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">processed</span><span class="p">,</span> <span class="n">chain</span><span class="p">)</span>
</span><span class='line'>  <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>このようにFilterを実現するためにPipelineという新しい仕組みを導入しているため、tagの書き換えによる多段フィルタをしなくて済むようになっている.</p>

<h2>まとめ</h2>

<ul>
<li>v0.12のLabel, Filterを実現している部分のコードを読んでみた</li>
<li>Labelの部分は、RootAgent(設定ファイルのROOT部分)および各labelディレクティブごとにルーティングテーブル(<code>EventRouter</code>)を分けることにより実現されている</li>
<li>Filterは、レコードに対する連続した処理を表現する、Pipelineという新たな仕組みを導入することで実現されている</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/28/incompatible-character-encodings-in-fluentd/">Fluentdで Incompatible Character Encodingのエラーが出た話</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-28T22:24:31+09:00" pubdate data-updated="true">Feb 28<span>th</span>, 2015</time>
        
           | <a href="/blog/2015/02/28/incompatible-character-encodings-in-fluentd/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2015/02/28/incompatible-character-encodings-in-fluentd/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Fluentdに対して、in_httpのインタフェースで送信してくるアプリケーションがあるのだけど、そのアプリケーションに対してリリースがあった後、一部のメッセージがうまく送れないという話があった. 見てみたら、受信側Fluentdに以下のようなメッセージが多発していた. fluentdのバージョンは0.10.48.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015-02-06 14:14:52 +0900 [warn]: fluent/engine.rb:156:rescue in emit_stream: em
</span><span class='line'>it transaction failed  error_class=Encoding::CompatibilityError error=#&lt;Encoding
</span><span class='line'>::CompatibilityError: incompatible character encodings: ASCII-8BIT and UTF-8&gt;
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/event.rb:32:in
</span><span class='line'>`&lt;&lt;'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/event.rb:32:in
</span><span class='line'>`to_msgpack'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/event.rb:32:in
</span><span class='line'>`block in to_msgpack_stream'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/event.rb:54:in
</span><span class='line'>`call'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/event.rb:54:in
</span><span class='line'>`each'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/event.rb:31:in
</span><span class='line'>`to_msgpack_stream'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/output.rb:424:i
</span><span class='line'>n `emit'
</span><span class='line'>  2015-02-06 14:14:52 +0900 [warn]: plugin/in_http.rb:147:on_request: /opt/td-ag
</span><span class='line'>ent/embedded/lib/ruby/gems/2.1.0/gems/fluentd-0.10.48/lib/fluent/output.rb:33:in
</span><span class='line'> `next'
</span></code></pre></td></tr></table></div></figure>


<p>全てのメッセージが送れてないわけではない模様. 適当なメッセージをcurlで送ってみても問題ないので
なんだろう、、、と思っていたら、以下のpull requestを見つけた. 偶然、事象が発生したのと同日.</p>

<p><a href="https://github.com/fluent/fluentd/pull/550">https://github.com/fluent/fluentd/pull/550</a></p>

<p>Messagepackの不具合?で、5KBを超える、且つエンコードがASCII-8BIT以外の文字列をシリアライズしようとした時に、このエラーが出ることがあるそうな. msgpack 0.5.11で修正.</p>

<p>アプリケーションの人に聞いてみると、確かにメッセージサイズは大幅に増えているようだし、エラー発生時のパケットを見ても、メッセージサイズが数百KBととても大きい. このpull requestは v0.10.60で取り込まれているようなので、td-agentを2.1.4に上げてテストをしてみたら発生しなくなった.</p>

<p>もっと前にこれが発生してたら困ってただろうな. ちょうど修正されてて助かりました.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/27/hdp2-dot-2-resoruce-manager-could-not-transition-to-active/">HDP2.2でResourceManagerが両系standbyになった事象</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-27T17:45:06+09:00" pubdate data-updated="true">Jan 27<span>th</span>, 2015</time>
        
           | <a href="/blog/2015/01/27/hdp2-dot-2-resoruce-manager-could-not-transition-to-active/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2015/01/27/hdp2-dot-2-resoruce-manager-could-not-transition-to-active/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>HDP2.2で、daemonやクラスタの再起動を繰り返していた所、ResourceManagerが両系standbyになってしまった.</p>

<p>ResourceManagerのログには以下のように出力されている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015-01-16 16:29:19,495 WARN  resourcemanager.RMAuditLogger
</span><span class='line'>(RMAuditLogger.java:logFailure(285)) - USER=yarn
</span><span class='line'>OPERATION=transitionToActive    TARGET=RMHAProtocolService
</span><span class='line'>RESULT=FAILURE  DESCRIPTION=Exception transitioning to active
</span><span class='line'>PERMISSIONS=All users are allowed
</span><span class='line'>2015-01-16 16:29:19,495 WARN  ha.ActiveStandbyElector
</span><span class='line'>(ActiveStandbyElector.java:becomeActive(809)) - Exception handling the
</span><span class='line'>winning of election
</span><span class='line'>org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)
</span><span class='line'>        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:805)
</span><span class='line'>        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:416)
</span><span class='line'>        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
</span><span class='line'>        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
</span><span class='line'>Caused by: org.apache.hadoop.ha.ServiceFailedException: Error when
</span><span class='line'>transitioning to Active mode
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:304)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)
</span><span class='line'>        ... 4 more
</span><span class='line'>Caused by: org.apache.hadoop.service.ServiceStateException:
</span><span class='line'>org.apache.hadoop.yarn.exceptions.YarnException: Application with id
</span><span class='line'>application_1421115867116_0001 is already present! Cannot add a
</span><span class='line'>duplicate!
</span><span class='line'>        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
</span><span class='line'>        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:204)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at javax.security.auth.Subject.doAs(Subject.java:415)
</span><span class='line'>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)
</span><span class='line'>        ... 5 more
</span><span class='line'>Caused by: org.apache.hadoop.yarn.exceptions.YarnException:
</span><span class='line'>Application with id application_1421115867116_0001 is already present!
</span><span class='line'>Cannot add a duplicate!
</span><span class='line'>        at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:338)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:309)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)
</span><span class='line'>        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)
</span><span class='line'>        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
</span></code></pre></td></tr></table></div></figure>


<p>Zookeeperに変なエントリが残ってる？と思われたのでRMを停止した上で<code>/rmstore/ZKRMStateRoot/RMAppRoot/</code>以下のApplicatoinIDのエントリを全て削除.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[zk: localhost:2181(CONNECTED) 2] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421387925857_0002
</span><span class='line'>[zk: localhost:2181(CONNECTED) 3] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0002
</span><span class='line'>[zk: localhost:2181(CONNECTED) 4] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0003
</span><span class='line'>[zk: localhost:2181(CONNECTED) 5] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0004
</span><span class='line'>[zk: localhost:2181(CONNECTED) 6] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421320519530_0002
</span><span class='line'>[zk: localhost:2181(CONNECTED) 7] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421115867116_0005
</span><span class='line'>[zk: localhost:2181(CONNECTED) 8] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421320519530_0001
</span><span class='line'>[zk: localhost:2181(CONNECTED) 11] rmr /rmstore/ZKRMStateRoot/RMAppRoot/application_1421387925857_0001
</span></code></pre></td></tr></table></div></figure>


<p>で、RMを起動したら復旧した.</p>

<p>どうやら<a href="https://issues.apache.org/jira/browse/YARN-2865">YARN-2865</a>の事象っぽい. Hadoop 2.7.0でFIX. ZK上のエントリを消してしまったので、その分のジョブについてはクライアントから再投入する必要がある.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/06/hdp2-dot-2-installation-memo/">HDP2.2をセットアップするためにハマった箇所のメモ</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-06T20:53:54+09:00" pubdate data-updated="true">Jan 6<span>th</span>, 2015</time>
        
           | <a href="/blog/2015/01/06/hdp2-dot-2-installation-memo/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2015/01/06/hdp2-dot-2-installation-memo/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>HDP2.2を手元のVMで試しにセットアップしてみたが、色々ハマった部分があったのでメモ</p>

<h2>環境</h2>

<p>CentOS6.3のVMを7つ用意して、以下のようにHA含めて構成することにした.</p>

<ul>
<li>master1: NameNode(active), ZKFC, JournalNode, Zookeeper</li>
<li>master2: NameNode(standby), ZKFC, JournalNode, ResourceManager(standby), Zookeeper</li>
<li>master3: JournalNode, ResourceManager(active), Zookeeper, HiveServer2, MySQL</li>
<li>slaves(3ノード): DataNode, NodeManager</li>
<li>client: MR/Tez client</li>
</ul>


<p>ドキュメントは、こちらの&#8221;Installing HDP Manually&#8221;を使用.
<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html</a></p>

<h2>トラブルシュートなどのメモ</h2>

<p>以下、ドキュメントには無いが変更しないといけなかったもの、引っかかったトラブルなど. 単に自分が手順を見落としていたり、間違っていたために発生したものもあるかも.</p>

<h3>全般</h3>

<ul>
<li>基本的に、設定はcompanion filesのものをベースにする. 2.1を動かしていた際の設定もあったが、大分変わっているようなので一旦companion filesのをまるごと持ってきた</li>
<li>インストールのベースが<code>/usr/hdp/2.2.0.0-2041/</code>になっているが、実際のスクリプトの中では<code>/usr/lib/hadoop</code>等を参照しているものもあるため、<code>/usr/hdp/2.2.0.0-2041/hadoop -&gt; /usr/hdp/2.2.0.0-2041/hadoop</code> 等のようなシンボリックリンクをひと通り作成した.</li>
<li>Daemonの起動スクリプト類は<code>hadoop-hdfs-namenode</code>等のような別RPMになっている.これらのインストール先は<code>/usr/hdp/2.2.0.0-2041/etc/</code>となっているため、<code>/etc/init.d</code>の下などに、こちらもシンボリックリンクを作成した. ちなみに、<code>/etc/default</code>の下に置くファイルも用意されているが、initスクリプトをみてもこれらを読むようには見えない.</li>
<li>マニュアルにはcompanion filesに含まれる、 <code>usersAndGroups.sh</code>, <code>directories.sh</code> を設定した上で <code>~/.bash_profile</code>でこれらのファイルを読む設定を入れるようにあるが、daemonの動作がbash_profileに依存するのが気持ち悪かったので、それはやってない. それに起因したトラブルもあるかも.</li>
</ul>


<h3>Zookeeperのセットアップ</h3>

<ul>
<li>initスクリプト内で呼ばれる、<code>zookeeper-server</code>,<code>zookeeper-server-initialize</code>は<code>/usr/hdp/2.2.0.0-2041/zookeeper/bin/</code>にあるので、これらを<code>/usr/bin</code>下に置くよう、シンボリックリンクを作成した</li>
</ul>


<h3>service <hadoop daemon> start の戻り値が3</h3>

<p>また、停止に失敗したりする.</p>

<p>以下の様な感じ.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># service hadoop-yarn-resourcemanager start
</span><span class='line'>Starting Hadoop resourcemanager:                           [  OK  ]
</span><span class='line'>starting resourcemanager, logging to /var/log/hadoop/yarn/yarn-yarn-resourcemanager-hdp15.hadoop.local.out
</span><span class='line'>[root@hdp15 init.d]# echo $?
</span><span class='line'>3</span></code></pre></td></tr></table></div></figure>


<p>initスクリプト内で以下のようにPIDFILEを設定しているが、環境変数が正しく設定されていないと、PIDFILEがうまく作られずにこのような状態になる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>PIDFILE="$HADOOP_PID_DIR/yarn-$YARN_IDENT_STRING-resourcemanager.pid"</span></code></pre></td></tr></table></div></figure>


<p><code>yarn-env.sh</code>で<code>HADOOP_PID_DIR</code>, <code>YARN_IDENT_STRING</code>, <code>hadoop-env.sh</code>でも<code>HADOOP_PID_DIR</code>を設定するようにした.</p>

<h3>HistoryServerでPermission Deninedが発生</h3>

<p>以下の様なエラーが発生. (何をしようとして発生したのか忘れた&hellip;)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-12-27 03:15:05,824 ERROR hs.HistoryFileManager (HistoryFileManager.java:scanIfNeeded(285)) - Error while trying to scan the directory hdfs://hdpexperiment:
</span><span class='line'>8020/mr-history/tmp/client
</span><span class='line'>org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=READ_EXECUTE, inode="/mr-history/tmp/client":client:hdfs:drwxrwx---
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:185)</span></code></pre></td></tr></table></div></figure>


<p>HDFSのパーミッションを見ると以下のようになっており、<code>/mr-history/tmp/client</code>に対して<code>mapred</code>ユーザのパーミッションが無い.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># sudo -u hdfs hdfs dfs -ls /
</span><span class='line'>Found 6 items
</span><span class='line'>drwxrwxrwt   - yarn   yarn            0 2014-12-27 03:11 /app-logs
</span><span class='line'>drwxr-xr-x   - hdfs   hdfs            0 2014-12-27 02:43 /apps
</span><span class='line'>drwxr-xr-x   - hdfs   hadoop          0 2014-12-26 16:17 /hdp
</span><span class='line'>drwxr-xr-x   - mapred hdfs            0 2014-12-26 16:16 /mr-history
</span><span class='line'>drwxrwxrwt   - hdfs   hdfs            0 2014-12-27 03:11 /tmp
</span><span class='line'>drwxr-xr-x   - hdfs   hdfs            0 2014-12-27 02:33 /user
</span><span class='line'># sudo -u hdfs hdfs dfs -ls /mr-history
</span><span class='line'>Found 2 items
</span><span class='line'>drwxrwxrwt   - mapred hdfs          0 2014-12-26 16:16 /mr-history/done
</span><span class='line'>drwxrwxrwt   - mapred hdfs          0 2014-12-26 23:27 /mr-history/tmp
</span><span class='line'># sudo -u hdfs hdfs dfs -ls /mr-history/tmp
</span><span class='line'>Found 1 items
</span><span class='line'>drwxrwx---   - client hdfs          0 2014-12-27 00:14 /mr-history/tmp/client
</span></code></pre></td></tr></table></div></figure>


<p>以下のように、<code>/mr-history</code>以下のgroupを<code>mapredと</code>することで対応.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># sudo -u hdfs hdfs dfs -chgrp -R mapred /mr-history</span></code></pre></td></tr></table></div></figure>


<h3>MapReduceジョブ実行中のエラー. Slaveにつながらない</h3>

<p>exampleのpiを実行した際のエラー</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>14/12/30 11:17:59 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:00 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:01 INFO ipc.Client: Retrying connect to server: hdp18.hadoop.local/10.29.254.69:39110. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:29 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:30 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:31 INFO ipc.Client: Retrying connect to server: hdp17.hadoop.local/10.29.254.67:43296. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
</span><span class='line'>14/12/30 11:18:31 INFO mapreduce.Job: Job job_1419895534181_0002 failed with state FAILED due to: Application application_1419895534181_0002 failed 2 times due to AM Container for appattempt_1419895534181_0002_000002 exited with  exitCode: 255
</span><span class='line'>For more detailed output, check application tracking page:http://hdp16.hadoop.local:8088/proxy/application_1419895534181_0002/Then, click on links to logs of each attempt.
</span><span class='line'>Diagnostics: Exception from container-launch.
</span><span class='line'>Container id: container_1419895534181_0002_02_000001
</span><span class='line'>Exit code: 255
</span><span class='line'>Stack trace: ExitCodeException exitCode=255: 
</span><span class='line'>        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
</span><span class='line'>        at org.apache.hadoop.util.Shell.run(Shell.java:455)
</span><span class='line'>        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
</span><span class='line'>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Container exited with a non-zero exit code 255
</span><span class='line'>Failing this attempt. Failing the application.
</span><span class='line'>14/12/30 11:18:31 INFO mapreduce.Job: Counters: 0
</span><span class='line'>Job Finished in 99.802 seconds
</span><span class='line'>java.io.FileNotFoundException: File does not exist: hdfs://hdpexperiment/user/hdfs/QuasiMonteCarlo_1419905807487_296085847/out/reduce-out
</span><span class='line'>        at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1122)
</span><span class='line'>        at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
</span><span class='line'>        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
</span><span class='line'>        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
</span><span class='line'>        at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1750)
</span><span class='line'>        at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1774)
</span><span class='line'>        at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:314)
</span><span class='line'>        at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:354)
</span><span class='line'>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
</span><span class='line'>        at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:363)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
</span><span class='line'>        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
</span><span class='line'>        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
</span><span class='line'>        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p><code>yarn logs</code>ではログが見えなかったので、実行中のサーバでコンテナのログを見てみた. <code>${hdp.version}</code>というのがそのままになっている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-12-30 13:08:49,568 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
</span><span class='line'>java.lang.IllegalArgumentException: Unable to parse '/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework' as a URI, check the setting for mapreduce.application.framework.path
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.getMRFrameworkName(MRApps.java:178)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.setMRFrameworkClasspath(MRApps.java:203)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.setClasspath(MRApps.java:248)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInitialClasspath(TaskAttemptImpl.java:620)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:755)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext(TaskAttemptImpl.java:812)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1527)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1504)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1069)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1311)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1303)
</span><span class='line'>        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
</span><span class='line'>        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>Caused by: java.net.URISyntaxException: Illegal character in path at index 11: /hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework
</span><span class='line'>        at java.net.URI$Parser.fail(URI.java:2829)
</span><span class='line'>        at java.net.URI$Parser.checkChars(URI.java:3002)
</span><span class='line'>        at java.net.URI$Parser.parseHierarchical(URI.java:3086)
</span><span class='line'>        at java.net.URI$Parser.parse(URI.java:3044)
</span><span class='line'>        at java.net.URI.&lt;init&gt;(URI.java:595)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.util.MRApps.getMRFrameworkName(MRApps.java:176)
</span><span class='line'>        ... 18 more
</span></code></pre></td></tr></table></div></figure>


<p><code>mapred-site.xml</code>で、<code>${hdp.version}</code>となっている部分を実際のバージョン番号(<code>2.2.0.0-2041</code>)に変えたら解消した.</p>

<h3>Journalnodeの停止に失敗</h3>

<p>journalnodeを停止すると、<code>no journalnode to stop</code>というエラーが発生. ただ、プロセスは停止している.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># service hadoop-hdfs-journalnode stop
</span><span class='line'>Stopping Hadoop journalnode:                               [  OK  ]
</span><span class='line'>no journalnode to stop
</span><span class='line'>rm: cannot remove `/var/run/hadoop/hadoop-hdfs-journalnode.pid': Permission denied
</span><span class='line'># ls -l /var/run/hadoop/hadoop-hdfs-journalnode.pid
</span><span class='line'>-rw-r--r-- 1 hdfs hdfs 6 12月 30 08:24 2014 /var/run/hadoop/hadoop-hdfs-journalnode.pid
</span><span class='line'># ps -ef | grep -i journal
</span><span class='line'>root       374     2  0 Dec29 ?        00:00:02 [kjournald]
</span><span class='line'>root       811     2  0 Dec29 ?        00:00:00 [kjournald]
</span><span class='line'>root     14406 14342  0 14:15 pts/0    00:00:00 grep -i journal</span></code></pre></td></tr></table></div></figure>


<p>ディレクトリのowner/groupが<code>mapred</code>になっている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ls -ld /var/run/hadoop
</span><span class='line'>drwxr-xr-x 5 mapred mapred 4096 12月 30 14:16 2014 /var/run/hadoop</span></code></pre></td></tr></table></div></figure>


<p>これを<code>hdfs.hdfs</code>にしたら事象は解消したが、HistoryServerをrestartしたら<code>/var/run/hadoop</code>のownerが<code>mapred.mapred</code>に戻ってしまった.</p>

<p><code>hadoop-mapreduce-historyserver</code>のinitスクリプトにある、以下の部分のせい. つまり、historyserverとHDFS系のdaemonが同居している場合に発生する問題.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>install -d -m 0755 -o mapred -g mapred $HADOOP_PID_DIR 1&gt;/dev/null 2&gt;&1 || :
</span><span class='line'>[ -d "$LOCKDIR" ] || install -d -m 0755 $LOCKDIR 1&gt;/dev/null 2&gt;&1 || :
</span></code></pre></td></tr></table></div></figure>


<p><code>hadoop-env.sh</code>に以下の記述を入れ、HistoryServerの<code>HADOOP_PID_DIR</code>をHDFS系と分けることで対応することにした.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># For HistoryServer
</span><span class='line'>if [ "${SVC_USER}" = "mapred" ]; then
</span><span class='line'>  HADOOP_PID_DIR=/var/run/hadoop-mapreduce
</span><span class='line'>fi
</span></code></pre></td></tr></table></div></figure>


<h3>Hive CREATE TABLE時のNo privilege</h3>

<p>beelineからhiveユーザで接続し、create tableを発行した際のエラー</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0: jdbc:hive2://hdp16.hadoop.local:10000&gt; create table test2(a int, b string); 
</span><span class='line'>Error: Error while compiling statement: FAILED: SemanticException MetaException(message:No privilege 'Select' found for inputs { database:default}) (state=42000,code=40000)</span></code></pre></td></tr></table></div></figure>


<p><a href="https://cwiki.apache.org/confluence/display/Hive/Storage+Based+Authorization+in+the+Metastore+Server">Storage Based Authorization in the Metastore Server</a>に引っかかっている模様</p>

<p>一旦以下の設定を外して対応</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener&lt;/value&gt;
</span><span class='line'>    &lt;description&gt;List of comma separated listeners for metastore events.&lt;/description&gt;
</span><span class='line'>  &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h3>Hive on MRでのクエリ実行エラー</h3>

<p>HiveServer2のログには以下のメッセージが出ている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015-01-06 03:14:42,300 ERROR [HiveServer2-Background-Pool: Thread-65]: exec.Task (SessionState.java:printError(833)) - Ended Job = job_1420474977406_0003 with
</span><span class='line'>exception 'java.lang.NumberFormatException(For input string: "100000L")'
</span><span class='line'>java.lang.NumberFormatException: For input string: "100000L"
</span><span class='line'>        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
</span><span class='line'>        at java.lang.Long.parseLong(Long.java:441)
</span><span class='line'>        at java.lang.Long.parseLong(Long.java:483)
</span><span class='line'>        at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1189)
</span><span class='line'>        at org.apache.hadoop.hive.conf.HiveConf.getLongVar(HiveConf.java:2253)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.checkFatalErrors(HadoopJobExecHelper.java:209)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:308)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:547)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:435)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1604)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1364)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1177)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)
</span><span class='line'>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:999)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:144)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:69)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:196)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at javax.security.auth.Subject.doAs(Subject.java:415)
</span><span class='line'>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
</span><span class='line'>        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)
</span><span class='line'>        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:208)
</span><span class='line'>        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
</span><span class='line'>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span></code></pre></td></tr></table></div></figure>


<p><a href="https://issues.apache.org/jira/browse/AMBARI-8219">AMBARI-8219</a>
の事例に従い、hive-site.xmlの以下を変更したらOK</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>   &lt;property&gt;
</span><span class='line'>     &lt;name&gt;hive.exec.max.created.files&lt;/name&gt;
</span><span class='line'>-    &lt;value&gt;100000L&lt;/value&gt;
</span><span class='line'>+    &lt;value&gt;100000&lt;/value&gt;
</span><span class='line'>     &lt;description&gt;Maximum number of HDFS files created by all mappers/reducers in a MapReduce job.&lt;/description&gt;
</span><span class='line'>   &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h3>hive.execution.engine=tez でのHiveクエリ実行エラー1</h3>

<p>HiveServer2のログは以下の通り</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>5841 end=1419741946024 duration=183 from=org.apache.hadoop.hive.ql.Driver&gt;
</span><span class='line'>2014-12-28 13:45:46,025 ERROR [HiveServer2-Handler-Pool: Thread-56]: thrift.ProcessFunction (ProcessFunction.java:process(41)) - Internal error processing ExecuteStatement
</span><span class='line'>java.lang.NoClassDefFoundError: org/apache/tez/dag/api/SessionNotRunning
</span><span class='line'>        at java.lang.Class.getDeclaredConstructors0(Native Method)
</span><span class='line'>        at java.lang.Class.privateGetDeclaredConstructors(Class.java:2585)
</span><span class='line'>        at java.lang.Class.getConstructor0(Class.java:2885)
</span><span class='line'>        at java.lang.Class.newInstance(Class.java:350)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.TaskFactory.get(TaskFactory.java:133)</span></code></pre></td></tr></table></div></figure>


<p>このクラスはtez-apiのjarに含まれているが、HiveServer2のサーバにTezクライアントをセットアップしていなかったのが原因だった. セットアップして解消.</p>

<h3>Tez OrderedWordCountの実行エラー</h3>

<p>Tezの動作確認として、tez-examplesに含まれる、OrderedWordCountを実行した際のエラー.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># su - hdfs
</span><span class='line'>-bash-4.1$ cat /tmp/test.txt 
</span><span class='line'>foo bar foo bar foo
</span><span class='line'>$ hadoop fs -put /tmp/test.txt /tmp/test.txt 
</span><span class='line'>-bash-4.1$ hadoop jar /usr/hdp/2.2.0.0-2041/tez/tez-examples-0.5.2.2.2.0.0-2041.jar orderedwordcount /tmp/test.txt /tmp/out
</span><span class='line'>Running OrderedWordCount
</span><span class='line'>14/12/27 02:54:57 INFO client.TezClient: Tez Client Version: [ component=tez-api, version=0.5.2.2.2.0.0-2041, revision=db32ad437885baf17ab90885b4ddb226fbbe3559, SCM-URL=scm:git:https://git-wip-us.apache.org/repos/asf/tez.git, buildTIme=20141119-1512 ]
</span><span class='line'>14/12/27 02:54:59 INFO client.TezClient: Submitting DAG application with id: application_1419606523103_0010
</span><span class='line'>14/12/27 02:54:59 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
</span><span class='line'>14/12/27 02:54:59 INFO client.TezClientUtils: Using tez.lib.uris value from configuration: hdfs://hdpexperiment/apps/tez/,hdfs://hdpexperiment/apps/tez/lib/,hdfs://hdpexperiment/hdp/apps/current/tez/tez.tar.gz
</span><span class='line'>14/12/27 02:54:59 WARN client.TezClientUtils: Duplicate resource found, resourceName=tez.tar.gz, existingPath=scheme: "hdfs" host: "hdpexperiment" port: -1 file: "/apps/tez/lib/tez.tar.gz", newPath=hdfs://hdpexperiment/hdp/apps/current/tez/tez.tar.gz
</span><span class='line'>14/12/27 02:54:59 INFO client.TezClient: Tez system stage directory hdfs://hdpexperiment/tmp/hdfs/staging/.tez/application_1419606523103_0010 doesn't exist and is created
</span><span class='line'>14/12/27 02:55:00 INFO client.TezClient: Submitting DAG to YARN, applicationId=application_1419606523103_0010, dagName=OrderedWordCount
</span><span class='line'>14/12/27 02:55:01 INFO impl.YarnClientImpl: Submitted application application_1419606523103_0010
</span><span class='line'>14/12/27 02:55:01 INFO client.TezClient: The url to track the Tez AM: http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/
</span><span class='line'>14/12/27 02:55:01 INFO client.DAGClientImpl: Waiting for DAG to start running
</span><span class='line'>14/12/27 02:55:13 INFO client.DAGClientImpl: DAG completed. FinalState=FAILED
</span><span class='line'>OrderedWordCount failed with diagnostics: [Application application_1419606523103_0010 failed 2 times due to AM Container for appattempt_1419606523103_0010_000002 exited with  exitCode: 1
</span><span class='line'>For more detailed output, check application tracking page:http://hdp16.hadoop.local:8088/proxy/application_1419606523103_0010/Then, click on links to logs of each attempt.
</span><span class='line'>Diagnostics: Exception from container-launch.
</span><span class='line'>Container id: container_1419606523103_0010_02_000001
</span><span class='line'>Exit code: 1
</span><span class='line'>Stack trace: ExitCodeException exitCode=1:
</span><span class='line'>        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
</span><span class='line'>        at org.apache.hadoop.util.Shell.run(Shell.java:455)
</span><span class='line'>        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
</span><span class='line'>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Container exited with a non-zero exit code 1
</span><span class='line'>Failing this attempt. Failing the application.]
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>コンテナのログを見ると以下の通り. クラスが見つからない.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Container: container_1419606523103_0009_01_000001 on hdp17.hadoop.local_45454
</span><span class='line'>===============================================================================
</span><span class='line'>LogType:stderr
</span><span class='line'>Log Upload Time:27-12-2014 03:37:52
</span><span class='line'>LogLength:1445
</span><span class='line'>Log Contents:
</span><span class='line'>Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/service/AbstractService
</span><span class='line'>        at java.lang.ClassLoader.defineClass1(Native Method)
</span><span class='line'>        at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
</span><span class='line'>        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
</span><span class='line'>        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
</span><span class='line'>        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)
</span><span class='line'>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.service.AbstractService
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>        ... 13 more
</span></code></pre></td></tr></table></div></figure>


<p>このクラスは、hadoop-common.jarに入っており、<code>yarn classpath</code>で確認するとこのjarもCLASSPATHに含まれているのだが、、、</p>

<p>(1/9追記)どうやら、<code>tez.lib.uris</code>の問題だった模様.以下のように、HDFSに乗せた<code>tez.tar.gz</code>のパスを指定したらうまく動作した.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;/hdp/apps/current/tez/tez.tar.gz&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h2>まとめ</h2>

<p>ということで、HDP2.2のクラスタを作ろうとして色々うまくいかなかったのでまとめてみた. <del>あとはTezのエラーを解消したいなぁ.</del> あと、マニュアルをブラウザで見るととても見づらいので、PDFをダウンロードして手元で見た方が良い.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/16/try-fluentd-v0-dot-12-at-least-once/">Fluentd v0.12のAt-least-once Semanticsを試す</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-12-16T00:34:17+09:00" pubdate data-updated="true">Dec 16<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/12/16/try-fluentd-v0-dot-12-at-least-once/#disqus_thread"
             data-disqus-identifier="http://ogibayashi.github.io/blog/2014/12/16/try-fluentd-v0-dot-12-at-least-once/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Fluentd v0.12のin/out_forwardでAt-least-once semanticsがサポートされるようになった. 今まではアプリケーションレイヤでの到達確認がなかったので、一部のネットワーク障害などのケースでは、送信されたように見えて実は送信されていない、という事象が発生し得た. v0.12から導入された<code>require_ack_response</code>オプションを使うと、このような事象を避けることができる.</p>

<p>この機能が導入されたpull requestはこちら.
<a href="https://github.com/fluent/fluentd/pull/428">https://github.com/fluent/fluentd/pull/428</a></p>

<p>ということで試してみた.</p>

<h2>require_ack_responseがない場合</h2>

<p>fluentd 0.10.56で試す. (0.12で試しても良かったのだけど..)</p>

<p>送信側は以下の設定. 相手先fluentdが早々にdetachされてしまうのを避けるため、<code>hard_timeout</code>と<code>phi_threshold</code>を入れた</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;source&gt;
</span><span class='line'>   type forward
</span><span class='line'>&lt;/source&gt;
</span><span class='line'>&lt;match test.**&gt;
</span><span class='line'>   type forward
</span><span class='line'>   flush_interval 1s
</span><span class='line'>   heartbeat_type tcp
</span><span class='line'>   hard_timeout 600
</span><span class='line'>   phi_threshold 300
</span><span class='line'>   buffer_type file
</span><span class='line'>   buffer_path /var/log/fluentd.*.buffer
</span><span class='line'>   &lt;server&gt;
</span><span class='line'>     host  192.168.1.2
</span><span class='line'>     port 24224
</span><span class='line'>   &lt;/server&gt;
</span><span class='line'>&lt;/match&gt;
</span></code></pre></td></tr></table></div></figure>


<p>受信側はこんな感じ</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;source&gt;
</span><span class='line'>  type forward
</span><span class='line'>&lt;/source&gt;
</span><span class='line'>
</span><span class='line'>&lt;match test.**&gt;
</span><span class='line'>  type file
</span><span class='line'>  path /tmp/fluentd_forward.log
</span><span class='line'>&lt;/match&gt;
</span></code></pre></td></tr></table></div></figure>


<p>で、パケットが届かないが、アプリケーションにエラーが返らない状況を作るため、受信側のiptablesでSYNが立っていないパケットをドロップするようにする. SYNは相手に到達し、SYN-ACKも返るため、アプリケーションからは正常に接続されている様に見えることになる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># iptables -A INPUT -p tcp --syn --dport 24224 -j ACCEPT
</span><span class='line'># iptables -A INPUT -p tcp --dport 24224 -j DROP</span></code></pre></td></tr></table></div></figure>


<p>これでログを送ってみる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># echo '{"aaa": 1}' | fluent-cat  test.data
</span><span class='line'># echo '{"bbb": 2}' | fluent-cat test.data</span></code></pre></td></tr></table></div></figure>


<p>netstatで送信側のソケットを見る. Send-Qにデータが溜まっている.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># netstat -na | grep 24224
</span><span class='line'>tcp        0      0 0.0.0.0:24224               0.0.0.0:*                   LISTEN
</span><span class='line'>tcp        0      1 192.168.1.1:10652          192.168.1.2:24224          FIN_WAIT1
</span><span class='line'>tcp        0     41 192.168.1.1:10655          192.168.1.2:24224          FIN_WAIT1
</span><span class='line'>udp        0      0 0.0.0.0:24224               0.0.0.0:*
</span></code></pre></td></tr></table></div></figure>


<p>しばらくすると、ソケットが破棄される.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># netstat -na | grep 24224
</span><span class='line'>tcp        0      0 0.0.0.0:24224               0.0.0.0:*                   LISTEN      
</span><span class='line'>tcp        0      1 192.168.1.1:10664           192.168.1.2:24224          FIN_WAIT1   
</span><span class='line'>udp        0      0 0.0.0.0:24224               0.0.0.0:*             
</span></code></pre></td></tr></table></div></figure>


<p>この状況だと、アプリケーション的には正常に送れているように見えてしまうので、バッファは削除される. つまりログがロストした状況.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ls /var/log/fluentd.*.buffer
</span><span class='line'>ls: cannot access /var/log/fluentd.*.buffer: そのようなファイルやディレクトリはありません
</span></code></pre></td></tr></table></div></figure>


<h2>require_ack_responseを使う</h2>

<p>次に、送受信共に<code>v0.12.1</code>にして、送信側に<code>require_ack_response</code>の設定を入れてみる.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;source&gt;
</span><span class='line'>    type forward
</span><span class='line'>  &lt;/source&gt;
</span><span class='line'>  &lt;match test.**&gt;
</span><span class='line'>    type forward
</span><span class='line'>    flush_interval 1s
</span><span class='line'>    heartbeat_type tcp
</span><span class='line'>    hard_timeout 600
</span><span class='line'>    phi_threshold 300
</span><span class='line'>    buffer_type file
</span><span class='line'>    buffer_path /var/log/fluentd.*.buffer
</span><span class='line'>    require_ack_response 
</span><span class='line'>    &lt;server&gt;
</span><span class='line'>      host 192.168.1.2
</span><span class='line'>      port 24224
</span><span class='line'>    &lt;/server&gt;
</span><span class='line'>  &lt;/match&gt;
</span></code></pre></td></tr></table></div></figure>


<p>同様にfluent-catで送る. 今度は、一定時間後に以下のようにエラーになった.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-12-15 15:25:56 +0900 [warn]: no response from 192.168.1.2:24224. regard it as unavailable.
</span><span class='line'>2014-12-15 15:26:56 +0900 [warn]: temporarily failed to flush the buffer. next_retry=2014-12-15 15:22:46 +0900 error_class="Fluent::ForwardOutputACKTimeoutError" error="node 10.29.254.66:24224 does not return ACK" plugin_id="object:16c7e3c"
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:321:in `send_data'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:169:in `block in write_objects'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:163:in `times'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/plugin/out_forward.rb:163:in `write_objects'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/output.rb:459:in `write'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/buffer.rb:325:in `write_chunk'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/buffer.rb:304:in `pop'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/output.rb:320:in `try_flush'
</span><span class='line'>  2014-12-15 15:26:56 +0900 [warn]: /usr/local/rvm/gems/ruby-2.1.5/gems/fluentd-0.12.1/lib/fluent/output.rb:140:in `run'
</span></code></pre></td></tr></table></div></figure>


<p>バッファも残っている</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ls /var/log/fluentd.test.data.*.buffer
</span><span class='line'>/var/log/fluentd.test.data.b50a3b457dcfed028.buffer  /var/log/fluentd.test.data.q50a3b455b1eac4ca.buffer</span></code></pre></td></tr></table></div></figure>


<p>しばらく放置した後、iptablesを解除したら無事に送信された.</p>

<h2>まとめ</h2>

<p>Fluentd v0.12で導入されたAt-least-once semanticsを試してみた. アプリケーションレイヤでの到達確認が実装されることで、TCPレイヤでパケットがうまく届いていないケースについても、fluentdがそれを検知して再送してくれることが確認できた.</p>

<p>ちなみに自分のところでは、ruby1.9上でfluentdを動かしていた時にプロセスが短時間ブロックするような事象が多発していて、それに起因してログのロストが発生したことがある. 恐らく、上記のようにTCPのコネクションは確立したように見えて、実は相手側がハング状態だったためにソケットバッファに滞留、最終的にソケットクローズ時にパケットが破棄されたのだと考えている.
(この時は、td-agent2にしたら解消した)</p>

<p><code>require_ack_response</code>により、そのようなケースでもfluentdがちゃんと検知して再送してくれるので、このオプションは是非入れておきたい.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/04/05/apache-flink-performance/">Apache Flinkの性能 - デフォルトのJSONパーサが遅かった話</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/03/25/gree-tech-talk-10/">GREE Tech Talk 10に行ってきた</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/02/26/trying-apache-flink/">Apache Flinkを試している</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/02/08/hadoop-slash-spark-conference-2016/">Hadoop / Spark Conference Japan 2016 に行ってきた</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/08/13/esper-and-java-method/">EsperのEPLでjava methodとGROUP Byを使ったらレコードが重複した話</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>
    <ul id="category-list"><li><a href='/blog/categories/docker'>Docker (1)</a></li><li><a href='/blog/categories/elasticsearch'>elasticsearch (1)</a></li><li><a href='/blog/categories/esper'>Esper (1)</a></li><li><a href='/blog/categories/flink'>Flink (2)</a></li><li><a href='/blog/categories/fluentd'>fluentd (4)</a></li><li><a href='/blog/categories/hadoop'>Hadoop (3)</a></li><li><a href='/blog/categories/norikra'>Norikra (1)</a></li><li><a href='/blog/categories/openshift'>openshift (1)</a></li><li><a href='/blog/categories/puppet'>puppet (1)</a></li><li><a href='/blog/categories/spark'>Spark (1)</a></li><li><a href='/blog/categories/mian-qiang-hui'>勉強会 (1)</a></li></ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - OGIBAYASHI Hironori -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'technotes-ogibayashi';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
